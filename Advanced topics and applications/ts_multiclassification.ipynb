{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from keras.optimizers import Adagrad\n",
    "from tslearn.shapelets import ShapeletModel\n",
    "from tslearn.shapelets import grabocka_params_to_shapelet_size_dict\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts0 = pd.read_csv(r'/Users/Cava/Desktop/University/Data Science & Business Informatics/Data Mining/Advanced Topics and Applications/Project/data/temperatureTS.csv')\n",
    "\n",
    "ts0 = ts0[ts0['Unnamed: 0'] < '2015-02-18 00:00:00']\n",
    "ts0= ts0[ts0['Unnamed: 0'] > '2015-02-02 23:59:59']\n",
    "\n",
    "ts = pd.Series(ts0['0'].values, ts0['Unnamed: 0'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X = scaler.fit_transform(ts).reshape(ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d6f11fefe63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaplet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ts, ts_sz = X_train.shape\n",
    "n_classes = len(set(y))\n",
    "\n",
    "# Set the number of shapelets per size as done in the original paper\n",
    "shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=n_ts,\n",
    "                                                       ts_sz=ts_sz,\n",
    "                                                       n_classes=n_classes,\n",
    "                                                       l=0.1,\n",
    "                                                       r=1)\n",
    "\n",
    "print('n_ts', n_ts)\n",
    "print('ts_sz', ts_sz)\n",
    "print('n_classes', n_classes)\n",
    "print('shapelet_sizes', shapelet_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using parameters provided by the authors (except that we use\n",
    "# fewer iterations here)\n",
    "shp_clf = ShapeletModel(n_shapelets_per_size=shapelet_sizes,\n",
    "                        optimizer=\"sgd\",\n",
    "                        weight_regularizer=.01,\n",
    "                        max_iter=200,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/riccardo/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/riccardo/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/riccardo/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/200\n",
      "700/700 [==============================] - 0s 157us/step - loss: 1.1601 - categorical_accuracy: 0.3200 - categorical_crossentropy: 1.1058\n",
      "Epoch 2/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1600 - categorical_accuracy: 0.3100 - categorical_crossentropy: 1.1058\n",
      "Epoch 3/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1599 - categorical_accuracy: 0.3200 - categorical_crossentropy: 1.1057\n",
      "Epoch 4/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1598 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1057\n",
      "Epoch 5/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1597 - categorical_accuracy: 0.3257 - categorical_crossentropy: 1.1057\n",
      "Epoch 6/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1596 - categorical_accuracy: 0.3257 - categorical_crossentropy: 1.1056\n",
      "Epoch 7/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1596 - categorical_accuracy: 0.3243 - categorical_crossentropy: 1.1056\n",
      "Epoch 8/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1595 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1056\n",
      "Epoch 9/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1594 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1056\n",
      "Epoch 10/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1593 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1055\n",
      "Epoch 11/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1592 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1055\n",
      "Epoch 12/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1591 - categorical_accuracy: 0.3257 - categorical_crossentropy: 1.1055\n",
      "Epoch 13/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1590 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1055\n",
      "Epoch 14/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1589 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1054\n",
      "Epoch 15/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1588 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1054\n",
      "Epoch 16/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1587 - categorical_accuracy: 0.3271 - categorical_crossentropy: 1.1054\n",
      "Epoch 17/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1586 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1053\n",
      "Epoch 18/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1585 - categorical_accuracy: 0.3243 - categorical_crossentropy: 1.1053\n",
      "Epoch 19/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1584 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1053\n",
      "Epoch 20/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1583 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1053\n",
      "Epoch 21/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1582 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1052\n",
      "Epoch 22/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1581 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1052\n",
      "Epoch 23/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1580 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1052\n",
      "Epoch 24/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1579 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1052\n",
      "Epoch 25/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1578 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1051\n",
      "Epoch 26/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1578 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1051\n",
      "Epoch 27/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1577 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1051\n",
      "Epoch 28/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1576 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1051\n",
      "Epoch 29/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1575 - categorical_accuracy: 0.3400 - categorical_crossentropy: 1.1050\n",
      "Epoch 30/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1574 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1050\n",
      "Epoch 31/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1573 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1050\n",
      "Epoch 32/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1572 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1050\n",
      "Epoch 33/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1571 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1049\n",
      "Epoch 34/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1570 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1049\n",
      "Epoch 35/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1569 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1049\n",
      "Epoch 36/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1569 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1049\n",
      "Epoch 37/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1568 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1049\n",
      "Epoch 38/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1567 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1048\n",
      "Epoch 39/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1566 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1048\n",
      "Epoch 40/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1565 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1048\n",
      "Epoch 41/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1564 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1048\n",
      "Epoch 42/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1563 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1047\n",
      "Epoch 43/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1562 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1047\n",
      "Epoch 44/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1561 - categorical_accuracy: 0.3414 - categorical_crossentropy: 1.1047\n",
      "Epoch 45/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1561 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1047\n",
      "Epoch 46/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1560 - categorical_accuracy: 0.3414 - categorical_crossentropy: 1.1046\n",
      "Epoch 47/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1559 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1046\n",
      "Epoch 48/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1558 - categorical_accuracy: 0.3400 - categorical_crossentropy: 1.1046\n",
      "Epoch 49/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1557 - categorical_accuracy: 0.3400 - categorical_crossentropy: 1.1046\n",
      "Epoch 50/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1556 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1046\n",
      "Epoch 51/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1555 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1045\n",
      "Epoch 52/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1554 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1045\n",
      "Epoch 53/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1554 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1045\n",
      "Epoch 54/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1553 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1045\n",
      "Epoch 55/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1552 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1045\n",
      "Epoch 56/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1551 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1044\n",
      "Epoch 57/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1550 - categorical_accuracy: 0.3443 - categorical_crossentropy: 1.1044\n",
      "Epoch 58/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1549 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1044\n",
      "Epoch 59/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1548 - categorical_accuracy: 0.3486 - categorical_crossentropy: 1.1044\n",
      "Epoch 60/200\n",
      "700/700 [==============================] - 0s 20us/step - loss: 1.1548 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1044\n",
      "Epoch 61/200\n",
      "700/700 [==============================] - 0s 24us/step - loss: 1.1547 - categorical_accuracy: 0.3486 - categorical_crossentropy: 1.1043\n",
      "Epoch 62/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1546 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1043\n",
      "Epoch 63/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1545 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1043\n",
      "Epoch 64/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1544 - categorical_accuracy: 0.3486 - categorical_crossentropy: 1.1043\n",
      "Epoch 65/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1543 - categorical_accuracy: 0.3471 - categorical_crossentropy: 1.1042\n",
      "Epoch 66/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1543 - categorical_accuracy: 0.3514 - categorical_crossentropy: 1.1042\n",
      "Epoch 67/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1542 - categorical_accuracy: 0.3443 - categorical_crossentropy: 1.1042\n",
      "Epoch 68/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1541 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1042\n",
      "Epoch 69/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1540 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1042\n",
      "Epoch 70/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1539 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1042\n",
      "Epoch 71/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1538 - categorical_accuracy: 0.3443 - categorical_crossentropy: 1.1041\n",
      "Epoch 72/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1538 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1041\n",
      "Epoch 73/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1537 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1041\n",
      "Epoch 74/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1536 - categorical_accuracy: 0.3414 - categorical_crossentropy: 1.1041\n",
      "Epoch 75/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1535 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1041\n",
      "Epoch 76/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1534 - categorical_accuracy: 0.3471 - categorical_crossentropy: 1.1040\n",
      "Epoch 77/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1534 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1040\n",
      "Epoch 78/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1533 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1040\n",
      "Epoch 79/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1532 - categorical_accuracy: 0.3443 - categorical_crossentropy: 1.1040\n",
      "Epoch 80/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1531 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1040\n",
      "Epoch 81/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1530 - categorical_accuracy: 0.3457 - categorical_crossentropy: 1.1039\n",
      "Epoch 82/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1529 - categorical_accuracy: 0.3343 - categorical_crossentropy: 1.1039\n",
      "Epoch 83/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1529 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1039\n",
      "Epoch 84/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1528 - categorical_accuracy: 0.3314 - categorical_crossentropy: 1.1039\n",
      "Epoch 85/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1527 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1038\n",
      "Epoch 86/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1526 - categorical_accuracy: 0.3400 - categorical_crossentropy: 1.1038\n",
      "Epoch 87/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1525 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1038\n",
      "Epoch 88/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1524 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1038\n",
      "Epoch 89/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1524 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1038\n",
      "Epoch 90/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1523 - categorical_accuracy: 0.3357 - categorical_crossentropy: 1.1038\n",
      "Epoch 91/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1522 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1037\n",
      "Epoch 92/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1521 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1037\n",
      "Epoch 93/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1521 - categorical_accuracy: 0.3429 - categorical_crossentropy: 1.1037\n",
      "Epoch 94/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1520 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1037\n",
      "Epoch 95/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1519 - categorical_accuracy: 0.3400 - categorical_crossentropy: 1.1037\n",
      "Epoch 96/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1518 - categorical_accuracy: 0.3371 - categorical_crossentropy: 1.1036\n",
      "Epoch 97/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1517 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1036\n",
      "Epoch 98/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1517 - categorical_accuracy: 0.3271 - categorical_crossentropy: 1.1036\n",
      "Epoch 99/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1516 - categorical_accuracy: 0.3286 - categorical_crossentropy: 1.1036\n",
      "Epoch 100/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1515 - categorical_accuracy: 0.3329 - categorical_crossentropy: 1.1036\n",
      "Epoch 101/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1514 - categorical_accuracy: 0.3386 - categorical_crossentropy: 1.1035\n",
      "Epoch 102/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1513 - categorical_accuracy: 0.3300 - categorical_crossentropy: 1.1035\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 13us/step - loss: 1.1513 - categorical_accuracy: 0.3186 - categorical_crossentropy: 1.1035\n",
      "Epoch 104/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1512 - categorical_accuracy: 0.3157 - categorical_crossentropy: 1.1035\n",
      "Epoch 105/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1511 - categorical_accuracy: 0.3171 - categorical_crossentropy: 1.1035\n",
      "Epoch 106/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1510 - categorical_accuracy: 0.3157 - categorical_crossentropy: 1.1035\n",
      "Epoch 107/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1509 - categorical_accuracy: 0.3100 - categorical_crossentropy: 1.1034\n",
      "Epoch 108/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1509 - categorical_accuracy: 0.3186 - categorical_crossentropy: 1.1034\n",
      "Epoch 109/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1508 - categorical_accuracy: 0.3014 - categorical_crossentropy: 1.1034\n",
      "Epoch 110/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1507 - categorical_accuracy: 0.2986 - categorical_crossentropy: 1.1034\n",
      "Epoch 111/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1506 - categorical_accuracy: 0.3014 - categorical_crossentropy: 1.1034\n",
      "Epoch 112/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1506 - categorical_accuracy: 0.3000 - categorical_crossentropy: 1.1034\n",
      "Epoch 113/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1505 - categorical_accuracy: 0.2900 - categorical_crossentropy: 1.1033\n",
      "Epoch 114/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1504 - categorical_accuracy: 0.2986 - categorical_crossentropy: 1.1033\n",
      "Epoch 115/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1503 - categorical_accuracy: 0.2914 - categorical_crossentropy: 1.1033\n",
      "Epoch 116/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1503 - categorical_accuracy: 0.2743 - categorical_crossentropy: 1.1033\n",
      "Epoch 117/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1502 - categorical_accuracy: 0.2814 - categorical_crossentropy: 1.1033\n",
      "Epoch 118/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1501 - categorical_accuracy: 0.2657 - categorical_crossentropy: 1.1032\n",
      "Epoch 119/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1500 - categorical_accuracy: 0.2771 - categorical_crossentropy: 1.1032\n",
      "Epoch 120/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1499 - categorical_accuracy: 0.2729 - categorical_crossentropy: 1.1032\n",
      "Epoch 121/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1499 - categorical_accuracy: 0.2714 - categorical_crossentropy: 1.1032\n",
      "Epoch 122/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1498 - categorical_accuracy: 0.2686 - categorical_crossentropy: 1.1032\n",
      "Epoch 123/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1497 - categorical_accuracy: 0.2657 - categorical_crossentropy: 1.1032\n",
      "Epoch 124/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1496 - categorical_accuracy: 0.2557 - categorical_crossentropy: 1.1031\n",
      "Epoch 125/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1496 - categorical_accuracy: 0.2643 - categorical_crossentropy: 1.1031\n",
      "Epoch 126/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1495 - categorical_accuracy: 0.2571 - categorical_crossentropy: 1.1031\n",
      "Epoch 127/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1494 - categorical_accuracy: 0.2600 - categorical_crossentropy: 1.1031\n",
      "Epoch 128/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1493 - categorical_accuracy: 0.2514 - categorical_crossentropy: 1.1031\n",
      "Epoch 129/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1493 - categorical_accuracy: 0.2557 - categorical_crossentropy: 1.1031\n",
      "Epoch 130/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1492 - categorical_accuracy: 0.2557 - categorical_crossentropy: 1.1030\n",
      "Epoch 131/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1491 - categorical_accuracy: 0.2543 - categorical_crossentropy: 1.1030\n",
      "Epoch 132/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1490 - categorical_accuracy: 0.2586 - categorical_crossentropy: 1.1030\n",
      "Epoch 133/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1490 - categorical_accuracy: 0.2557 - categorical_crossentropy: 1.1030\n",
      "Epoch 134/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1489 - categorical_accuracy: 0.2500 - categorical_crossentropy: 1.1030\n",
      "Epoch 135/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1488 - categorical_accuracy: 0.2443 - categorical_crossentropy: 1.1030\n",
      "Epoch 136/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1487 - categorical_accuracy: 0.2557 - categorical_crossentropy: 1.1030\n",
      "Epoch 137/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1487 - categorical_accuracy: 0.2500 - categorical_crossentropy: 1.1029\n",
      "Epoch 138/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1486 - categorical_accuracy: 0.2500 - categorical_crossentropy: 1.1029\n",
      "Epoch 139/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1485 - categorical_accuracy: 0.2543 - categorical_crossentropy: 1.1029\n",
      "Epoch 140/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1484 - categorical_accuracy: 0.2386 - categorical_crossentropy: 1.1029\n",
      "Epoch 141/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1484 - categorical_accuracy: 0.2414 - categorical_crossentropy: 1.1029\n",
      "Epoch 142/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1483 - categorical_accuracy: 0.2357 - categorical_crossentropy: 1.1029\n",
      "Epoch 143/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1482 - categorical_accuracy: 0.2286 - categorical_crossentropy: 1.1028\n",
      "Epoch 144/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1482 - categorical_accuracy: 0.2271 - categorical_crossentropy: 1.1028\n",
      "Epoch 145/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1481 - categorical_accuracy: 0.2086 - categorical_crossentropy: 1.1028\n",
      "Epoch 146/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1480 - categorical_accuracy: 0.2329 - categorical_crossentropy: 1.1028\n",
      "Epoch 147/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1479 - categorical_accuracy: 0.2229 - categorical_crossentropy: 1.1028\n",
      "Epoch 148/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1479 - categorical_accuracy: 0.2186 - categorical_crossentropy: 1.1028\n",
      "Epoch 149/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1478 - categorical_accuracy: 0.2286 - categorical_crossentropy: 1.1027\n",
      "Epoch 150/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1477 - categorical_accuracy: 0.2257 - categorical_crossentropy: 1.1027\n",
      "Epoch 151/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1476 - categorical_accuracy: 0.2086 - categorical_crossentropy: 1.1027\n",
      "Epoch 152/200\n",
      "700/700 [==============================] - 0s 18us/step - loss: 1.1476 - categorical_accuracy: 0.2114 - categorical_crossentropy: 1.1027\n",
      "Epoch 153/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1475 - categorical_accuracy: 0.1943 - categorical_crossentropy: 1.1027\n",
      "Epoch 154/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1474 - categorical_accuracy: 0.2086 - categorical_crossentropy: 1.1027\n",
      "Epoch 155/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1474 - categorical_accuracy: 0.2086 - categorical_crossentropy: 1.1027\n",
      "Epoch 156/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1473 - categorical_accuracy: 0.2014 - categorical_crossentropy: 1.1026\n",
      "Epoch 157/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1472 - categorical_accuracy: 0.2171 - categorical_crossentropy: 1.1026\n",
      "Epoch 158/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1471 - categorical_accuracy: 0.2243 - categorical_crossentropy: 1.1026\n",
      "Epoch 159/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1471 - categorical_accuracy: 0.2086 - categorical_crossentropy: 1.1026\n",
      "Epoch 160/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1470 - categorical_accuracy: 0.2029 - categorical_crossentropy: 1.1026\n",
      "Epoch 161/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1469 - categorical_accuracy: 0.2100 - categorical_crossentropy: 1.1026\n",
      "Epoch 162/200\n",
      "700/700 [==============================] - 0s 19us/step - loss: 1.1469 - categorical_accuracy: 0.2057 - categorical_crossentropy: 1.1025\n",
      "Epoch 163/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1468 - categorical_accuracy: 0.1957 - categorical_crossentropy: 1.1025\n",
      "Epoch 164/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1467 - categorical_accuracy: 0.2014 - categorical_crossentropy: 1.1025\n",
      "Epoch 165/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1467 - categorical_accuracy: 0.1943 - categorical_crossentropy: 1.1025\n",
      "Epoch 166/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1466 - categorical_accuracy: 0.1857 - categorical_crossentropy: 1.1025\n",
      "Epoch 167/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1465 - categorical_accuracy: 0.1957 - categorical_crossentropy: 1.1025\n",
      "Epoch 168/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1464 - categorical_accuracy: 0.1857 - categorical_crossentropy: 1.1024\n",
      "Epoch 169/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1464 - categorical_accuracy: 0.1886 - categorical_crossentropy: 1.1024\n",
      "Epoch 170/200\n",
      "700/700 [==============================] - 0s 18us/step - loss: 1.1463 - categorical_accuracy: 0.1900 - categorical_crossentropy: 1.1024\n",
      "Epoch 171/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1462 - categorical_accuracy: 0.1871 - categorical_crossentropy: 1.1024\n",
      "Epoch 172/200\n",
      "700/700 [==============================] - 0s 17us/step - loss: 1.1462 - categorical_accuracy: 0.1814 - categorical_crossentropy: 1.1024\n",
      "Epoch 173/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1461 - categorical_accuracy: 0.1857 - categorical_crossentropy: 1.1024\n",
      "Epoch 174/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1460 - categorical_accuracy: 0.1843 - categorical_crossentropy: 1.1024\n",
      "Epoch 175/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1460 - categorical_accuracy: 0.1771 - categorical_crossentropy: 1.1024\n",
      "Epoch 176/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1459 - categorical_accuracy: 0.1829 - categorical_crossentropy: 1.1023\n",
      "Epoch 177/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1458 - categorical_accuracy: 0.1786 - categorical_crossentropy: 1.1023\n",
      "Epoch 178/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1457 - categorical_accuracy: 0.1786 - categorical_crossentropy: 1.1023\n",
      "Epoch 179/200\n",
      "700/700 [==============================] - 0s 13us/step - loss: 1.1457 - categorical_accuracy: 0.1800 - categorical_crossentropy: 1.1023\n",
      "Epoch 180/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1456 - categorical_accuracy: 0.1800 - categorical_crossentropy: 1.1023\n",
      "Epoch 181/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1455 - categorical_accuracy: 0.1757 - categorical_crossentropy: 1.1023\n",
      "Epoch 182/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1455 - categorical_accuracy: 0.1771 - categorical_crossentropy: 1.1022\n",
      "Epoch 183/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1454 - categorical_accuracy: 0.1814 - categorical_crossentropy: 1.1022\n",
      "Epoch 184/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1453 - categorical_accuracy: 0.1743 - categorical_crossentropy: 1.1022\n",
      "Epoch 185/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1453 - categorical_accuracy: 0.1729 - categorical_crossentropy: 1.1022\n",
      "Epoch 186/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1452 - categorical_accuracy: 0.1657 - categorical_crossentropy: 1.1022\n",
      "Epoch 187/200\n",
      "700/700 [==============================] - 0s 12us/step - loss: 1.1451 - categorical_accuracy: 0.1700 - categorical_crossentropy: 1.1022\n",
      "Epoch 188/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1451 - categorical_accuracy: 0.1771 - categorical_crossentropy: 1.1022\n",
      "Epoch 189/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1450 - categorical_accuracy: 0.1629 - categorical_crossentropy: 1.1021\n",
      "Epoch 190/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1449 - categorical_accuracy: 0.1686 - categorical_crossentropy: 1.1021\n",
      "Epoch 191/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1449 - categorical_accuracy: 0.1686 - categorical_crossentropy: 1.1021\n",
      "Epoch 192/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1448 - categorical_accuracy: 0.1643 - categorical_crossentropy: 1.1021\n",
      "Epoch 193/200\n",
      "700/700 [==============================] - 0s 14us/step - loss: 1.1447 - categorical_accuracy: 0.1657 - categorical_crossentropy: 1.1021\n",
      "Epoch 194/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1447 - categorical_accuracy: 0.1643 - categorical_crossentropy: 1.1021\n",
      "Epoch 195/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1446 - categorical_accuracy: 0.1614 - categorical_crossentropy: 1.1021\n",
      "Epoch 196/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1445 - categorical_accuracy: 0.1671 - categorical_crossentropy: 1.1021\n",
      "Epoch 197/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1445 - categorical_accuracy: 0.1614 - categorical_crossentropy: 1.1020\n",
      "Epoch 198/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1444 - categorical_accuracy: 0.1686 - categorical_crossentropy: 1.1020\n",
      "Epoch 199/200\n",
      "700/700 [==============================] - 0s 15us/step - loss: 1.1443 - categorical_accuracy: 0.1614 - categorical_crossentropy: 1.1020\n",
      "Epoch 200/200\n",
      "700/700 [==============================] - 0s 16us/step - loss: 1.1443 - categorical_accuracy: 0.1629 - categorical_crossentropy: 1.1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeletModel(batch_size=256, max_iter=200, n_shapelets_per_size={12: 5},\n",
       "              optimizer='sgd', random_state=None, shapelet_length=0.15,\n",
       "              total_lengths=3, verbose=1, verbose_level=None,\n",
       "              weight_regularizer=0.01)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 103us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = shp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.12\n",
      "F1-score [0.05447471 0.31788079 0.05208333]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.04      0.07      0.05       100\n",
      "           1       0.47      0.24      0.32       100\n",
      "           2       0.05      0.05      0.05       100\n",
      "\n",
      "    accuracy                           0.12       300\n",
      "   macro avg       0.19      0.12      0.14       300\n",
      "weighted avg       0.19      0.12      0.14       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaplet-distances-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700/700 [==============================] - 0s 43us/step\n"
     ]
    }
   ],
   "source": [
    "X_train2 = shp_clf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00642236, 0.01045328, 0.03184402, 0.00556008, 0.00921067],\n",
       "       [0.00526969, 0.00396355, 0.04040583, 0.00768598, 0.011156  ],\n",
       "       [0.01017316, 0.00830499, 0.04240378, 0.01234547, 0.01212241],\n",
       "       ...,\n",
       "       [0.00514301, 0.01160882, 0.0261356 , 0.01025216, 0.04187443],\n",
       "       [0.01015361, 0.00490552, 0.03830056, 0.00912492, 0.01281242],\n",
       "       [0.01353542, 0.00951477, 0.01631339, 0.00850169, 0.01002848]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 10us/step\n"
     ]
    }
   ],
   "source": [
    "X_test2 = shp_clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.73\n",
      "F1-score [0.99       0.62616822 0.56989247]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       100\n",
      "           1       0.59      0.67      0.63       100\n",
      "           2       0.62      0.53      0.57       100\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.73      0.73      0.73       300\n",
      "weighted avg       0.73      0.73      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.69\n",
      "F1-score [0.96969697 0.57798165 0.52173913]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97       100\n",
      "           1       0.53      0.63      0.58       100\n",
      "           2       0.57      0.48      0.52       100\n",
      "\n",
      "    accuracy                           0.69       300\n",
      "   macro avg       0.69      0.69      0.69       300\n",
      "weighted avg       0.69      0.69      0.69       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(values):\n",
    "    features = {\n",
    "        'avg': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'var': np.var(values),\n",
    "        'med': np.median(values),\n",
    "        '10p': np.percentile(values, 10),\n",
    "        '25p': np.percentile(values, 25),\n",
    "        '50p': np.percentile(values, 50),\n",
    "        '75p': np.percentile(values, 75),\n",
    "        '90p': np.percentile(values, 90),\n",
    "        'iqr': np.percentile(values, 75) - np.percentile(values, 25),\n",
    "        'cov': 1.0 * np.mean(values) / np.std(values),\n",
    "        'skw': stats.skew(values),\n",
    "        'kur': stats.kurtosis(values)\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = np.array([list(calculate_features(x).values()) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 13)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39859446,  0.23519191,  0.05531524, ...,  1.69476262,\n",
       "         0.36034213, -0.8842845 ],\n",
       "       [ 0.45011623,  0.21610026,  0.04669932, ...,  2.08290465,\n",
       "         0.32862626, -0.61767966],\n",
       "       [ 0.42219464,  0.22458385,  0.05043791, ...,  1.87989759,\n",
       "         0.27583263, -0.91630096],\n",
       "       ...,\n",
       "       [ 0.52655729,  0.28183322,  0.07942997, ...,  1.86832936,\n",
       "        -0.46340955, -1.14728378],\n",
       "       [ 0.45154156,  0.22410403,  0.05022261, ...,  2.0148748 ,\n",
       "         0.19409557, -0.63827757],\n",
       "       [ 0.44552984,  0.20088531,  0.04035491, ...,  2.21783187,\n",
       "         0.41476313, -0.20365056]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test3 = np.array([list(calculate_features(x).values()) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6733333333333333\n",
      "F1-score [1.         0.45555556 0.55454545]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       0.51      0.41      0.46       100\n",
      "           2       0.51      0.61      0.55       100\n",
      "\n",
      "    accuracy                           0.67       300\n",
      "   macro avg       0.67      0.67      0.67       300\n",
      "weighted avg       0.67      0.67      0.67       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train3, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test3)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       100\n",
      "           2       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9733333333333334\n",
      "F1-score [0.95918367 0.99       0.97058824]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96       100\n",
      "           1       0.99      0.99      0.99       100\n",
      "           2       0.95      0.99      0.97       100\n",
      "\n",
      "    accuracy                           0.97       300\n",
      "   macro avg       0.97      0.97      0.97       300\n",
      "weighted avg       0.97      0.97      0.97       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.classification import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       100\n",
      "           2       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00       300\n",
      "   macro avg       1.00      1.00      1.00       300\n",
      "weighted avg       1.00      1.00      1.00       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(metric='dtw_sakoechiba')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, Activation, Conv1D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn(n_timesteps, n_outputs):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  128\n",
      "N. LABELS:  3\n"
     ]
    }
   ],
   "source": [
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train, test_size=0.2, stratify=y_train)\n",
    "\n",
    "n_timesteps, n_outputs, n_features = X_train_cnn.shape[1], len(np.unique(y_train_cnn)), 1 \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS: \", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_simple_cnn(n_timesteps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 121, 16)           144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 121, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 121, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 121, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 117, 32)           2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 117, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 117, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 117, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 115, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 115, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 115, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 115, 64)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 9,587\n",
      "Trainable params: 9,363\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train_cnn.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/5\n",
      "560/560 [==============================] - 1s 2ms/step - loss: 0.9407 - accuracy: 0.6089 - val_loss: 1.0650 - val_accuracy: 0.3357\n",
      "Epoch 2/5\n",
      "560/560 [==============================] - 0s 457us/step - loss: 0.7007 - accuracy: 0.8750 - val_loss: 1.0346 - val_accuracy: 0.3643\n",
      "Epoch 3/5\n",
      "560/560 [==============================] - 0s 490us/step - loss: 0.5671 - accuracy: 0.9179 - val_loss: 1.0371 - val_accuracy: 0.5929\n",
      "Epoch 4/5\n",
      "560/560 [==============================] - 0s 464us/step - loss: 0.4895 - accuracy: 0.9393 - val_loss: 1.0501 - val_accuracy: 0.4214\n",
      "Epoch 5/5\n",
      "560/560 [==============================] - 0s 463us/step - loss: 0.4054 - accuracy: 0.9536 - val_loss: 1.0500 - val_accuracy: 0.3357\n"
     ]
    }
   ],
   "source": [
    "history_cnn = cnn.fit(X_train_cnn, y_train_cnn, epochs=5, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.34\n",
      "F1-score [0.         0.50890585 0.03738318]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       100\n",
      "           1       0.34      1.00      0.51       100\n",
      "           2       0.29      0.02      0.04       100\n",
      "\n",
      "    accuracy                           0.34       300\n",
      "   macro avg       0.21      0.34      0.18       300\n",
      "weighted avg       0.21      0.34      0.18       300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn.predict(X_test_cnn), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 90us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0486841090520223, 0.3400000035762787]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(n_timesteps, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(n_timesteps, 1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = build_lstm(n_timesteps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 280,835\n",
      "Trainable params: 280,835\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 560 samples, validate on 140 samples\n",
      "Epoch 1/10\n",
      "560/560 [==============================] - 0s 494us/step - loss: 0.3096 - accuracy: 0.9786 - val_loss: 1.0019 - val_accuracy: 0.4786\n",
      "Epoch 2/10\n",
      "560/560 [==============================] - 0s 483us/step - loss: 0.2031 - accuracy: 0.9982 - val_loss: 0.9529 - val_accuracy: 0.4857\n",
      "Epoch 3/10\n",
      "560/560 [==============================] - 0s 477us/step - loss: 0.1474 - accuracy: 0.9946 - val_loss: 0.8271 - val_accuracy: 0.8286\n",
      "Epoch 4/10\n",
      "560/560 [==============================] - 0s 481us/step - loss: 0.1153 - accuracy: 0.9964 - val_loss: 0.6623 - val_accuracy: 0.9643\n",
      "Epoch 5/10\n",
      "560/560 [==============================] - 0s 476us/step - loss: 0.0897 - accuracy: 0.9946 - val_loss: 0.5490 - val_accuracy: 0.9571\n",
      "Epoch 6/10\n",
      "560/560 [==============================] - 0s 449us/step - loss: 0.0679 - accuracy: 0.9964 - val_loss: 0.3763 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "560/560 [==============================] - 0s 468us/step - loss: 0.0645 - accuracy: 0.9964 - val_loss: 0.3197 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "560/560 [==============================] - 0s 455us/step - loss: 0.0634 - accuracy: 0.9964 - val_loss: 0.2139 - val_accuracy: 0.9929\n",
      "Epoch 9/10\n",
      "560/560 [==============================] - 0s 452us/step - loss: 0.0481 - accuracy: 0.9964 - val_loss: 0.2339 - val_accuracy: 0.8714\n",
      "Epoch 10/10\n",
      "560/560 [==============================] - 0s 435us/step - loss: 0.0365 - accuracy: 0.9982 - val_loss: 0.1714 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "history_lstm = cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                       validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.3333333333333333\n",
      "F1-score [0.  0.5 0. ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       100\n",
      "           1       0.33      1.00      0.50       100\n",
      "           2       0.00      0.00      0.00       100\n",
      "\n",
      "    accuracy                           0.33       300\n",
      "   macro avg       0.11      0.33      0.17       300\n",
      "weighted avg       0.11      0.33      0.17       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm.predict(X_test_cnn), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0984290631612141, 0.3333333432674408]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.evaluate(X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import load_basic_motions\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 6, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_basic_motions(return_X_y=True)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'Standing', b'Standing', b'Standing', b'Standing', b'Standing',\n",
       "       b'Standing', b'Standing', b'Standing', b'Standing', b'Standing',\n",
       "       b'Running', b'Running', b'Running', b'Running', b'Running',\n",
       "       b'Running', b'Running', b'Running', b'Running', b'Running',\n",
       "       b'Walking', b'Walking', b'Walking', b'Walking', b'Walking',\n",
       "       b'Walking', b'Walking', b'Walking', b'Walking', b'Walking',\n",
       "       b'Badminton', b'Badminton', b'Badminton', b'Badminton',\n",
       "       b'Badminton', b'Badminton', b'Badminton', b'Badminton',\n",
       "       b'Badminton', b'Badminton'], dtype='|S12')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(40, 100, 6)\n",
    "X_test = X_test.reshape(40, 100, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  100\n",
      "N. LABELS:  4\n",
      "N. FEATURES:  6\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_outputs, n_features = X_train.shape[1], len(np.unique(y_train)), X_train.shape[2] \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS: \", n_outputs)\n",
    "print(\"N. FEATURES: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, Conv2D, MaxPool2D, Flatten, Dropout, LeakyReLU, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm2(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(n_timesteps, n_features), return_sequences=True, \n",
    "                        kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #1\n",
    "    for _ in range(2):\n",
    "        model.add(LSTM(4, kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.04))   \n",
    "\n",
    "    #2\n",
    "    model.add(LSTM(32, kernel_initializer='TruncatedNormal', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "    \n",
    "    #3\n",
    "    for _ in range(2):\n",
    "        model.add(Dense(256, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))\n",
    "    #4\n",
    "    for _ in range(1):\n",
    "        model.add(Dense(64, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.7))\n",
    "\n",
    "    #5\n",
    "    model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = build_lstm2(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 100, 4)            176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 100, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100, 4)            144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 100, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100, 4)            144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 100, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 100, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 32)                4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 100,708\n",
      "Trainable params: 99,404\n",
      "Non-trainable params: 1,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 8 samples\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 1.6396 - accuracy: 0.2500 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 1.3807 - accuracy: 0.4062 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 1.4749 - accuracy: 0.2188 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4223 - accuracy: 0.2188 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.3451 - accuracy: 0.4062 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 1.4631 - accuracy: 0.2500 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4985 - accuracy: 0.2188 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3982 - accuracy: 0.3438 - val_loss: 1.3865 - val_accuracy: 0.2500\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3987 - accuracy: 0.3125 - val_loss: 1.3865 - val_accuracy: 0.2500\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.5110 - accuracy: 0.2812 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.2976 - accuracy: 0.4375 - val_loss: 1.3865 - val_accuracy: 0.2500\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.5131 - accuracy: 0.3438 - val_loss: 1.3867 - val_accuracy: 0.2500\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4397 - accuracy: 0.4688 - val_loss: 1.3869 - val_accuracy: 0.2500\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.3529 - accuracy: 0.3125 - val_loss: 1.3870 - val_accuracy: 0.2500\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4446 - accuracy: 0.3125 - val_loss: 1.3872 - val_accuracy: 0.2500\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.3606 - accuracy: 0.3125 - val_loss: 1.3872 - val_accuracy: 0.2500\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.2436 - accuracy: 0.3125 - val_loss: 1.3875 - val_accuracy: 0.2500\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4079 - accuracy: 0.2812 - val_loss: 1.3879 - val_accuracy: 0.2500\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.2856 - accuracy: 0.3125 - val_loss: 1.3885 - val_accuracy: 0.2500\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.4732 - accuracy: 0.2812 - val_loss: 1.3890 - val_accuracy: 0.2500\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.3294 - accuracy: 0.5312 - val_loss: 1.3897 - val_accuracy: 0.2500\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1.2545 - accuracy: 0.4062 - val_loss: 1.3906 - val_accuracy: 0.2500\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.2595 - accuracy: 0.2500 - val_loss: 1.3913 - val_accuracy: 0.2500\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.2807 - accuracy: 0.3125 - val_loss: 1.3923 - val_accuracy: 0.2500\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.2815 - accuracy: 0.2500 - val_loss: 1.3929 - val_accuracy: 0.2500\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.3232 - accuracy: 0.3125 - val_loss: 1.3940 - val_accuracy: 0.2500\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.5695 - accuracy: 0.1875 - val_loss: 1.3953 - val_accuracy: 0.2500\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.2730 - accuracy: 0.3438 - val_loss: 1.3958 - val_accuracy: 0.2500\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.4051 - accuracy: 0.2812 - val_loss: 1.3969 - val_accuracy: 0.2500\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.3944 - accuracy: 0.3750 - val_loss: 1.3968 - val_accuracy: 0.2500\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.2970 - accuracy: 0.3125 - val_loss: 1.3969 - val_accuracy: 0.2500\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.3477 - accuracy: 0.4375 - val_loss: 1.3967 - val_accuracy: 0.2500\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.4630 - accuracy: 0.3125 - val_loss: 1.3967 - val_accuracy: 0.2500\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.1793 - accuracy: 0.3750 - val_loss: 1.3965 - val_accuracy: 0.2500\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3082 - accuracy: 0.3750 - val_loss: 1.3964 - val_accuracy: 0.2500\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.4891 - accuracy: 0.1875 - val_loss: 1.3966 - val_accuracy: 0.2500\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.2156 - accuracy: 0.3750 - val_loss: 1.3966 - val_accuracy: 0.2500\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3283 - accuracy: 0.4375 - val_loss: 1.3959 - val_accuracy: 0.2500\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.1527 - accuracy: 0.4688 - val_loss: 1.3961 - val_accuracy: 0.2500\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3443 - accuracy: 0.5000 - val_loss: 1.3959 - val_accuracy: 0.2500\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.2797 - accuracy: 0.3750 - val_loss: 1.3972 - val_accuracy: 0.2500\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.2660 - accuracy: 0.4062 - val_loss: 1.3964 - val_accuracy: 0.2500\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 1.4138 - accuracy: 0.3125 - val_loss: 1.3966 - val_accuracy: 0.2500\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1.4468 - accuracy: 0.3125 - val_loss: 1.3972 - val_accuracy: 0.2500\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 1.3009 - accuracy: 0.3438 - val_loss: 1.3966 - val_accuracy: 0.2500\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.2730 - accuracy: 0.5000 - val_loss: 1.3968 - val_accuracy: 0.2500\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 1.1731 - accuracy: 0.4062 - val_loss: 1.3969 - val_accuracy: 0.2500\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.2492 - accuracy: 0.4375 - val_loss: 1.3967 - val_accuracy: 0.2500\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.1663 - accuracy: 0.4688 - val_loss: 1.3977 - val_accuracy: 0.2500\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 1.3673 - accuracy: 0.4062 - val_loss: 1.3955 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "history_lstm2 = lstm2.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                          validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.25\n",
      "F1-score [0.  0.  0.  0.4]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.00      0.00      0.00        10\n",
      "           3       0.25      1.00      0.40        10\n",
      "\n",
      "    accuracy                           0.25        40\n",
      "   macro avg       0.06      0.25      0.10        40\n",
      "weighted avg       0.06      0.25      0.10        40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/riccardo/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm2.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 100, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 100, 6, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn2 = X_train_cnn.reshape(X_train_cnn.shape[0], X_train_cnn.shape[1], X_train_cnn.shape[2], 1)\n",
    "X_val_cnn2 = X_val_cnn.reshape(X_val_cnn.shape[0], X_val_cnn.shape[1], X_val_cnn.shape[2], 1)\n",
    "X_test_cnn2 = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "X_train_cnn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn2(n_timesteps, n_features, n_outputs):\n",
    "    input_shape = (n_timesteps, n_features, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ks1_first = 3\n",
    "    ks1_second = 3\n",
    "    \n",
    "    ks2_first = 4\n",
    "    ks2_second = 4\n",
    "    \n",
    "    model.add(Conv2D(filters=(3), \n",
    "                     kernel_size=(ks1_first, ks1_second),\n",
    "                     input_shape=input_shape, \n",
    "                     padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.02))\n",
    "    \n",
    "    for _ in range(2):\n",
    "        model.add(Conv2D(filters=(4), \n",
    "                     kernel_size= (ks2_first, ks2_second), \n",
    "                         padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))  \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(4):\n",
    "        model.add(Dense(64 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.4))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        model.add(Dense(128 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(1024 , kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = build_cnn2(n_timesteps, n_features, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 6, 3)         30        \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 100, 6, 3)         12        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 100, 6, 3)         0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 100, 6, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 6, 4)         196       \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 100, 6, 4)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 100, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 100, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 100, 6, 4)         260       \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 100, 6, 4)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 100, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 100, 6, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2400)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                153664    \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 350,870\n",
      "Trainable params: 347,520\n",
      "Non-trainable params: 3,350\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 8 samples\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 74ms/step - loss: 1.8272 - accuracy: 0.1562 - val_loss: 1.3863 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.6504 - accuracy: 0.3438 - val_loss: 1.3864 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.6615 - accuracy: 0.2812 - val_loss: 1.3865 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.7045 - accuracy: 0.2812 - val_loss: 1.3867 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 2.1631 - accuracy: 0.1875 - val_loss: 1.3868 - val_accuracy: 0.2500\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.6088 - accuracy: 0.2188 - val_loss: 1.3869 - val_accuracy: 0.2500\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.7161 - accuracy: 0.2812 - val_loss: 1.3872 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.6336 - accuracy: 0.3438 - val_loss: 1.3875 - val_accuracy: 0.2500\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.4444 - accuracy: 0.2812 - val_loss: 1.3882 - val_accuracy: 0.2500\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.7097 - accuracy: 0.3750 - val_loss: 1.3884 - val_accuracy: 0.2500\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.4574 - accuracy: 0.3750 - val_loss: 1.3879 - val_accuracy: 0.2500\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.8774 - accuracy: 0.2188 - val_loss: 1.3884 - val_accuracy: 0.2500\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5676 - accuracy: 0.2812 - val_loss: 1.3890 - val_accuracy: 0.2500\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.1729 - accuracy: 0.3438 - val_loss: 1.3897 - val_accuracy: 0.2500\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.4196 - accuracy: 0.3750 - val_loss: 1.3903 - val_accuracy: 0.2500\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3558 - accuracy: 0.3125 - val_loss: 1.3913 - val_accuracy: 0.2500\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5615 - accuracy: 0.3438 - val_loss: 1.3929 - val_accuracy: 0.2500\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4723 - accuracy: 0.2812 - val_loss: 1.3945 - val_accuracy: 0.2500\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4228 - accuracy: 0.2500 - val_loss: 1.3959 - val_accuracy: 0.2500\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.6014 - accuracy: 0.1562 - val_loss: 1.3948 - val_accuracy: 0.2500\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3968 - accuracy: 0.3125 - val_loss: 1.3953 - val_accuracy: 0.2500\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4730 - accuracy: 0.3438 - val_loss: 1.3968 - val_accuracy: 0.2500\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4776 - accuracy: 0.2812 - val_loss: 1.3985 - val_accuracy: 0.2500\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3880 - accuracy: 0.3125 - val_loss: 1.3994 - val_accuracy: 0.2500\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3957 - accuracy: 0.3438 - val_loss: 1.4017 - val_accuracy: 0.2500\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3530 - accuracy: 0.3438 - val_loss: 1.4039 - val_accuracy: 0.2500\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5212 - accuracy: 0.2500 - val_loss: 1.4042 - val_accuracy: 0.2500\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4691 - accuracy: 0.3750 - val_loss: 1.4024 - val_accuracy: 0.2500\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5327 - accuracy: 0.1562 - val_loss: 1.4012 - val_accuracy: 0.2500\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4108 - accuracy: 0.2812 - val_loss: 1.3965 - val_accuracy: 0.2500\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3100 - accuracy: 0.2812 - val_loss: 1.3927 - val_accuracy: 0.2500\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4827 - accuracy: 0.2812 - val_loss: 1.3873 - val_accuracy: 0.2500\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4206 - accuracy: 0.5312 - val_loss: 1.3842 - val_accuracy: 0.2500\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4358 - accuracy: 0.1875 - val_loss: 1.3803 - val_accuracy: 0.2500\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4078 - accuracy: 0.4062 - val_loss: 1.3749 - val_accuracy: 0.2500\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.3828 - accuracy: 0.4375 - val_loss: 1.3715 - val_accuracy: 0.2500\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.3863 - accuracy: 0.4688 - val_loss: 1.3679 - val_accuracy: 0.2500\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1948 - accuracy: 0.3750 - val_loss: 1.3635 - val_accuracy: 0.2500\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.4415 - accuracy: 0.3125 - val_loss: 1.3606 - val_accuracy: 0.2500\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3037 - accuracy: 0.3125 - val_loss: 1.3612 - val_accuracy: 0.2500\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3122 - accuracy: 0.4062 - val_loss: 1.3595 - val_accuracy: 0.2500\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.1783 - accuracy: 0.5625 - val_loss: 1.3552 - val_accuracy: 0.2500\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5319 - accuracy: 0.5000 - val_loss: 1.3500 - val_accuracy: 0.3750\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2696 - accuracy: 0.5312 - val_loss: 1.3442 - val_accuracy: 0.3750\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1187 - accuracy: 0.4688 - val_loss: 1.3347 - val_accuracy: 0.3750\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.0927 - accuracy: 0.5625 - val_loss: 1.3282 - val_accuracy: 0.3750\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.3168 - accuracy: 0.3750 - val_loss: 1.3186 - val_accuracy: 0.3750\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2313 - accuracy: 0.5625 - val_loss: 1.3058 - val_accuracy: 0.3750\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.2542 - accuracy: 0.6250 - val_loss: 1.2985 - val_accuracy: 0.3750\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.1461 - accuracy: 0.4375 - val_loss: 1.2980 - val_accuracy: 0.3750\n"
     ]
    }
   ],
   "source": [
    "history_cnn2 = cnn2.fit(X_train_cnn2, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn2, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.4\n",
      "F1-score [0.45454545 0.         0.75       0.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      1.00      0.45        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       1.00      0.60      0.75        10\n",
      "           3       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.40        40\n",
      "   macro avg       0.32      0.40      0.30        40\n",
      "weighted avg       0.32      0.40      0.30        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn2.predict(X_test_cnn2), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn3(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = build_cnn3(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 93, 16)            784       \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 93, 16)            64        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 93, 16)            0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 93, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 89, 32)            2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 89, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 89, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 89, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 87, 64)            6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 87, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 87, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 87, 64)            0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 4)                 260       \n",
      "=================================================================\n",
      "Total params: 10,292\n",
      "Trainable params: 10,068\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 8 samples\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.4343 - accuracy: 0.2500 - val_loss: 1.3885 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3860 - accuracy: 0.2812 - val_loss: 1.3847 - val_accuracy: 0.2500\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3422 - accuracy: 0.2812 - val_loss: 1.3822 - val_accuracy: 0.2500\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.3020 - accuracy: 0.4062 - val_loss: 1.3811 - val_accuracy: 0.2500\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.2780 - accuracy: 0.4375 - val_loss: 1.3802 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2428 - accuracy: 0.4375 - val_loss: 1.3804 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2198 - accuracy: 0.4688 - val_loss: 1.3798 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1464 - accuracy: 0.5938 - val_loss: 1.3791 - val_accuracy: 0.3750\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0697 - accuracy: 0.7188 - val_loss: 1.3778 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0818 - accuracy: 0.6562 - val_loss: 1.3767 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.0874 - accuracy: 0.6562 - val_loss: 1.3734 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9054 - accuracy: 0.8438 - val_loss: 1.3693 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9326 - accuracy: 0.7500 - val_loss: 1.3625 - val_accuracy: 0.6250\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8309 - accuracy: 0.8750 - val_loss: 1.3550 - val_accuracy: 0.6250\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7892 - accuracy: 0.8750 - val_loss: 1.3440 - val_accuracy: 0.6250\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8413 - accuracy: 0.7812 - val_loss: 1.3328 - val_accuracy: 0.6250\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7660 - accuracy: 0.8125 - val_loss: 1.3188 - val_accuracy: 0.6250\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7846 - accuracy: 0.8125 - val_loss: 1.2973 - val_accuracy: 0.6250\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7034 - accuracy: 0.8750 - val_loss: 1.2783 - val_accuracy: 0.6250\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6947 - accuracy: 0.8750 - val_loss: 1.2593 - val_accuracy: 0.7500\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.9688 - val_loss: 1.2273 - val_accuracy: 0.6250\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.9062 - val_loss: 1.1912 - val_accuracy: 0.6250\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.9688 - val_loss: 1.1596 - val_accuracy: 0.6250\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5593 - accuracy: 0.8438 - val_loss: 1.1349 - val_accuracy: 0.6250\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5922 - accuracy: 0.8438 - val_loss: 1.1213 - val_accuracy: 0.6250\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.9375 - val_loss: 1.0962 - val_accuracy: 0.7500\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.9375 - val_loss: 1.0408 - val_accuracy: 0.7500\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8750 - val_loss: 0.9920 - val_accuracy: 0.7500\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3840 - accuracy: 0.9375 - val_loss: 0.9457 - val_accuracy: 0.7500\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.9062 - val_loss: 0.9050 - val_accuracy: 0.7500\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.9375 - val_loss: 0.8623 - val_accuracy: 0.7500\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4206 - accuracy: 0.9062 - val_loss: 0.8375 - val_accuracy: 0.6250\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.9062 - val_loss: 0.8319 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.9062 - val_loss: 0.8456 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.9688 - val_loss: 0.8310 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.9062 - val_loss: 0.8008 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2431 - accuracy: 1.0000 - val_loss: 0.7833 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4630 - accuracy: 0.8750 - val_loss: 0.7612 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9688 - val_loss: 0.7423 - val_accuracy: 0.6250\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.6250\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8750 - val_loss: 0.7251 - val_accuracy: 0.7500\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.9688 - val_loss: 0.7432 - val_accuracy: 0.7500\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.9062 - val_loss: 0.7250 - val_accuracy: 0.7500\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 1.0000 - val_loss: 0.6891 - val_accuracy: 0.7500\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 0.7500\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3864 - accuracy: 0.9062 - val_loss: 0.6429 - val_accuracy: 0.8750\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 1.0000 - val_loss: 0.6281 - val_accuracy: 0.8750\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2391 - accuracy: 0.9375 - val_loss: 0.6253 - val_accuracy: 0.6250\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4578 - accuracy: 0.9062 - val_loss: 0.6614 - val_accuracy: 0.6250\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.9375 - val_loss: 0.7002 - val_accuracy: 0.6250\n"
     ]
    }
   ],
   "source": [
    "history_cnn3 = cnn3.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.75\n",
      "F1-score [0.57142857 0.76190476 0.72727273 0.86956522]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.40      0.57        10\n",
      "           1       0.73      0.80      0.76        10\n",
      "           2       0.67      0.80      0.73        10\n",
      "           3       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.75        40\n",
      "   macro avg       0.79      0.75      0.73        40\n",
      "weighted avg       0.79      0.75      0.73        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn3.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pyts.readthedocs.io/en/stable/generated/pyts.multivariate.classification.MultivariateClassifier.html#pyts.multivariate.classification.MultivariateClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
