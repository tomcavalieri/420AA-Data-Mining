{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "#plt.rcParams['figure.dpi']= 300  #resolution\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_list(ts_list, attribute, ts_number=1000, time = None , formatter = None):\n",
    "    if ts_number >= len(ts_list): ts_number = len(ts_list)-1\n",
    "    fig = plt.figure()\n",
    "    ax = fig.subplots()\n",
    "    for ts in ts_list[:ts_number]:\n",
    "        if time == None: index = list(ts.index.values)\n",
    "        else: index= ts[time]\n",
    "        ax.plot(index, ts[attribute])\n",
    "    ax.set_ylabel(attribute)\n",
    "    if formatter !=None:\n",
    "        date_form = DateFormatter(formatter)\n",
    "        ax.xaxis.set_major_formatter(date_form)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_col = 'date'\n",
    "\n",
    "attributes = ['date', 'Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Occupancy']\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'/Users/Cava/Desktop/University/Data Science & Business Informatics/Data Mining/Advanced Topics and Applications/Project/data/datatraining.txt')\n",
    "df[time_col] =  pd.to_datetime(df[time_col], format = '%Y-%m-%d %H:%M:%S')\n",
    "df['Weekday']=df[time_col].apply(lambda x:x.weekday())\n",
    "\n",
    "mon1=df[df['Weekday']==0][attributes].copy()\n",
    "tue1=df[df['Weekday']==1][attributes].copy()  #missing part of the day\n",
    "wed1=df[df['Weekday']==2][attributes].copy() #incompleted too\n",
    "thu1=df[df['Weekday']==3][attributes].copy()\n",
    "fri1=df[df['Weekday']==4][attributes].copy()\n",
    "sat1=df[df['Weekday']==5][attributes].copy()\n",
    "sun1=df[df['Weekday']==6][attributes].copy()\n",
    "    \n",
    "weekdays1 = [thu1,fri1,sat1,sun1,mon1] #I dont want incompleted days, so wednesday and tuesday are excluded\n",
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv(r'/Users/Cava/Desktop/University/Data Science & Business Informatics/Data Mining/Advanced Topics and Applications/Project/data/datatest2.txt')\n",
    "df[time_col] =  pd.to_datetime(df[time_col], format = '%Y-%m-%d %H:%M:%S')\n",
    "df['Weekday']=df[time_col].apply(lambda x:x.weekday())\n",
    "                                 \n",
    "mon2=df[df['Weekday']==0][attributes].copy()\n",
    "tue2=df[df['Weekday']==1][attributes].copy()  #missing part of the day\n",
    "wed2=df[df['Weekday']==2][attributes].copy() #incompleted too\n",
    "thu2=df[df['Weekday']==3][attributes].copy()\n",
    "fri2=df[df['Weekday']==4][attributes].copy()\n",
    "sat2=df[df['Weekday']==5][attributes].copy()\n",
    "sun2=df[df['Weekday']==6][attributes].copy()\n",
    "    \n",
    "weekdays2 = [thu2,fri2,sat2,sun2,mon2] #I dont want incompleted days, so wednesday and tuesday are excluded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "week_size = 144\n",
    "week_list = []\n",
    "labels = []\n",
    "\n",
    "week1 = pd.DataFrame()\n",
    "for day in weekdays1:\n",
    "    day.sort_values(time_col, inplace=True)#first of all: order\n",
    "    week1 = week1.append(day, ignore_index=True )    ##a week\n",
    "    \n",
    "    \n",
    "week1_list = []\n",
    "tmp = week1.copy() \n",
    "while(len(tmp)>=week_size):\n",
    "    ts = tmp.sample(week_size)\n",
    "    ts.sort_values(time_col, inplace=True)\n",
    "    week1_list.append(ts)\n",
    "    week_list.append(ts)\n",
    "    tmp.drop(ts.index, axis=0, inplace=True) #remove the sample just extract\n",
    "\n",
    "    \n",
    "week2 = pd.DataFrame()\n",
    "for day in weekdays2:    \n",
    "    day.sort_values(time_col, inplace=True)#first of all: order\n",
    "    week2 = week2.append(day, ignore_index=True)    ##a week\n",
    "    \n",
    "week2_list = []\n",
    "tmp = week2.copy() \n",
    "while(len(tmp)>=week_size):\n",
    "    ts = tmp.sample(week_size)\n",
    "    ts.sort_values(time_col, inplace=True)\n",
    "    week2_list.append(ts)\n",
    "    week_list.append(ts)\n",
    "    tmp.drop(ts.index, axis=0, inplace=True) #remove the sample just extract\n",
    "\n",
    "    \n",
    "random.shuffle(week_list)\n",
    "draw_list(week_list, 'CO2', 15, time = time_col ,formatter = '%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 144)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_list = []\n",
    "for temp in week_list:\n",
    "    ts = temp['Temperature'].copy()\n",
    "    ts_list.append((ts-ts.mean())/ts.std())\n",
    "\n",
    "ts_array = []\n",
    "ts_size = len(week_list[1])\n",
    "for ts in ts_list:\n",
    "    ts_array.append(np.array(ts).reshape(ts_size,))\n",
    "    \n",
    "X = np.array(ts_array)\n",
    "week1o2 = lambda ts : 0 if ts['date'][ts.index[0]].day == 12 else 1\n",
    "labels = [week1o2(ts) for ts in week_list]\n",
    "y = labels.copy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adagrad\n",
    "from tslearn.shapelets import ShapeletModel\n",
    "from tslearn.shapelets import grabocka_params_to_shapelet_size_dict\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X = scaler.fit_transform(X).reshape(X.shape[0], X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaplet Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ts 70\n",
      "ts_sz 144\n",
      "n_classes 2\n",
      "shapelet_sizes {14: 3}\n"
     ]
    }
   ],
   "source": [
    "n_ts, ts_sz = X_train.shape\n",
    "n_classes = len(set(y))\n",
    "\n",
    "# Set the number of shapelets per size as done in the original paper\n",
    "shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=n_ts,\n",
    "                                                       ts_sz=ts_sz,\n",
    "                                                       n_classes=n_classes,\n",
    "                                                       l=0.1,\n",
    "                                                       r=1)\n",
    "\n",
    "print('n_ts', n_ts)\n",
    "print('ts_sz', ts_sz)\n",
    "print('n_classes', n_classes)\n",
    "print('shapelet_sizes', shapelet_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using parameters provided by the authors (except that we use\n",
    "# fewer iterations here)\n",
    "shp_clf = ShapeletModel(n_shapelets_per_size=shapelet_sizes,\n",
    "                        optimizer=\"sgd\",\n",
    "                        weight_regularizer=.01,\n",
    "                        max_iter=200,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "70/70 [==============================] - 0s 86us/step - loss: 0.7181 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 2/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7181 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 3/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7181 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 4/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7181 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 5/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 6/200\n",
      "70/70 [==============================] - 0s 42us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 7/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 8/200\n",
      "70/70 [==============================] - 0s 38us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 9/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 10/200\n",
      "70/70 [==============================] - 0s 39us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 11/200\n",
      "70/70 [==============================] - 0s 42us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 12/200\n",
      "70/70 [==============================] - 0s 42us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 13/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 14/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7180 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 15/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 16/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 17/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 18/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 19/200\n",
      "70/70 [==============================] - 0s 43us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 20/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 21/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 22/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 23/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 24/200\n",
      "70/70 [==============================] - 0s 53us/step - loss: 0.7179 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 25/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 26/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 27/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 28/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 29/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 30/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 31/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 32/200\n",
      "70/70 [==============================] - 0s 38us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 33/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 34/200\n",
      "70/70 [==============================] - 0s 42us/step - loss: 0.7178 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 35/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 36/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 37/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 38/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 39/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 40/200\n",
      "70/70 [==============================] - 0s 40us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 41/200\n",
      "70/70 [==============================] - 0s 39us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 42/200\n",
      "70/70 [==============================] - 0s 39us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 43/200\n",
      "70/70 [==============================] - 0s 43us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 44/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7177 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 45/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 46/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 47/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 48/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 49/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 50/200\n",
      "70/70 [==============================] - 0s 66us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 51/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 52/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 53/200\n",
      "70/70 [==============================] - 0s 59us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 54/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7176 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 55/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 56/200\n",
      "70/70 [==============================] - 0s 68us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 57/200\n",
      "70/70 [==============================] - 0s 79us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 58/200\n",
      "70/70 [==============================] - 0s 72us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 59/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 60/200\n",
      "70/70 [==============================] - 0s 67us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "70/70 [==============================] - 0s 61us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 62/200\n",
      "70/70 [==============================] - 0s 67us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6933\n",
      "Epoch 63/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 64/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7175 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 65/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 66/200\n",
      "70/70 [==============================] - 0s 59us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 67/200\n",
      "70/70 [==============================] - 0s 64us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 68/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 69/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 70/200\n",
      "70/70 [==============================] - 0s 66us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 71/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 72/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 73/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 74/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7174 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 75/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 76/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 77/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 78/200\n",
      "70/70 [==============================] - 0s 57us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 79/200\n",
      "70/70 [==============================] - 0s 76us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 80/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 81/200\n",
      "70/70 [==============================] - 0s 43us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 82/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 83/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 84/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7173 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 85/200\n",
      "70/70 [==============================] - 0s 59us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 86/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 87/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 88/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 89/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 90/200\n",
      "70/70 [==============================] - 0s 59us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 91/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 92/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 93/200\n",
      "70/70 [==============================] - 0s 57us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 94/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 95/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7172 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 96/200\n",
      "70/70 [==============================] - 0s 66us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 97/200\n",
      "70/70 [==============================] - 0s 74us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 98/200\n",
      "70/70 [==============================] - 0s 40us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 99/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 100/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 101/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 102/200\n",
      "70/70 [==============================] - 0s 39us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 103/200\n",
      "70/70 [==============================] - 0s 66us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 104/200\n",
      "70/70 [==============================] - 0s 62us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 105/200\n",
      "70/70 [==============================] - 0s 57us/step - loss: 0.7171 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 106/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 107/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 108/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 109/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 110/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 111/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 112/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 113/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 114/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 115/200\n",
      "70/70 [==============================] - 0s 73us/step - loss: 0.7170 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 116/200\n",
      "70/70 [==============================] - 0s 37us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 117/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 118/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 119/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 120/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 122/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 123/200\n",
      "70/70 [==============================] - 0s 57us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 124/200\n",
      "70/70 [==============================] - 0s 74us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 125/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 126/200\n",
      "70/70 [==============================] - 0s 43us/step - loss: 0.7169 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 127/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 128/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 129/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 130/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 131/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 132/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 133/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 134/200\n",
      "70/70 [==============================] - 0s 60us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 135/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 136/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7168 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 137/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 138/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 139/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 140/200\n",
      "70/70 [==============================] - 0s 69us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 141/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 142/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 143/200\n",
      "70/70 [==============================] - 0s 62us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 144/200\n",
      "70/70 [==============================] - 0s 42us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 145/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 146/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7167 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 147/200\n",
      "70/70 [==============================] - 0s 67us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 148/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 149/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 150/200\n",
      "70/70 [==============================] - 0s 53us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 151/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 152/200\n",
      "70/70 [==============================] - 0s 60us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 153/200\n",
      "70/70 [==============================] - 0s 57us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 154/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 155/200\n",
      "70/70 [==============================] - 0s 40us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 156/200\n",
      "70/70 [==============================] - 0s 41us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 157/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7166 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 158/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 159/200\n",
      "70/70 [==============================] - 0s 46us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 160/200\n",
      "70/70 [==============================] - 0s 76us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 161/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 162/200\n",
      "70/70 [==============================] - 0s 60us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 163/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7165 - binary_accuracy: 0.5000 - binary_crossentropy: 0.6932\n",
      "Epoch 164/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7165 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 165/200\n",
      "70/70 [==============================] - 0s 54us/step - loss: 0.7165 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 166/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7165 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 167/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7165 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 168/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7165 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 169/200\n",
      "70/70 [==============================] - 0s 50us/step - loss: 0.7164 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 170/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7164 - binary_accuracy: 0.4857 - binary_crossentropy: 0.6932\n",
      "Epoch 171/200\n",
      "70/70 [==============================] - 0s 45us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 172/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 173/200\n",
      "70/70 [==============================] - 0s 51us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 174/200\n",
      "70/70 [==============================] - 0s 58us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 175/200\n",
      "70/70 [==============================] - 0s 44us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 176/200\n",
      "70/70 [==============================] - 0s 56us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 177/200\n",
      "70/70 [==============================] - 0s 75us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 178/200\n",
      "70/70 [==============================] - 0s 68us/step - loss: 0.7164 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 179/200\n",
      "70/70 [==============================] - 0s 73us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200\n",
      "70/70 [==============================] - 0s 82us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 181/200\n",
      "70/70 [==============================] - 0s 77us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 182/200\n",
      "70/70 [==============================] - 0s 72us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 183/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 184/200\n",
      "70/70 [==============================] - 0s 48us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 185/200\n",
      "70/70 [==============================] - 0s 53us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 186/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7163 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 187/200\n",
      "70/70 [==============================] - 0s 49us/step - loss: 0.7163 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 188/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7163 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 189/200\n",
      "70/70 [==============================] - 0s 52us/step - loss: 0.7163 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 190/200\n",
      "70/70 [==============================] - 0s 60us/step - loss: 0.7162 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 191/200\n",
      "70/70 [==============================] - 0s 55us/step - loss: 0.7162 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 192/200\n",
      "70/70 [==============================] - 0s 64us/step - loss: 0.7162 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 193/200\n",
      "70/70 [==============================] - 0s 78us/step - loss: 0.7162 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 194/200\n",
      "70/70 [==============================] - 0s 76us/step - loss: 0.7162 - binary_accuracy: 0.4429 - binary_crossentropy: 0.6932\n",
      "Epoch 195/200\n",
      "70/70 [==============================] - 0s 75us/step - loss: 0.7162 - binary_accuracy: 0.4571 - binary_crossentropy: 0.6932\n",
      "Epoch 196/200\n",
      "70/70 [==============================] - 0s 40us/step - loss: 0.7162 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 197/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7162 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 198/200\n",
      "70/70 [==============================] - 0s 65us/step - loss: 0.7162 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 199/200\n",
      "70/70 [==============================] - 0s 47us/step - loss: 0.7162 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n",
      "Epoch 200/200\n",
      "70/70 [==============================] - 0s 53us/step - loss: 0.7161 - binary_accuracy: 0.4714 - binary_crossentropy: 0.6932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ShapeletModel(max_iter=200, n_shapelets_per_size={14: 3}, verbose=1,\n",
       "              weight_regularizer=0.01)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shp_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = shp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5666666666666667\n",
      "F1-score [0.23529412 0.69767442]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.13      0.24        15\n",
      "           1       0.54      1.00      0.70        15\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.77      0.57      0.47        30\n",
      "weighted avg       0.77      0.57      0.47        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaplet-distances-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "70/70 [==============================] - 0s 18us/step\n"
     ]
    }
   ],
   "source": [
    "X_train2 = shp_clf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00961492, 0.00904149, 0.010555  ],\n",
       "       [0.0074772 , 0.00729009, 0.00863088],\n",
       "       [0.00542986, 0.00502806, 0.00601648],\n",
       "       [0.00373622, 0.00282966, 0.00368353],\n",
       "       [0.00295409, 0.00242596, 0.00334106],\n",
       "       [0.00517565, 0.00418332, 0.00541746],\n",
       "       [0.00945592, 0.00922877, 0.00997508],\n",
       "       [0.00295299, 0.0028081 , 0.00296651],\n",
       "       [0.00817058, 0.00736873, 0.00860783],\n",
       "       [0.00711666, 0.00610721, 0.00778168],\n",
       "       [0.00434842, 0.00424016, 0.00500733],\n",
       "       [0.00541822, 0.00542023, 0.00528584],\n",
       "       [0.00596032, 0.00628975, 0.0074315 ],\n",
       "       [0.00169024, 0.00145656, 0.00216301],\n",
       "       [0.0058855 , 0.00583454, 0.00662166],\n",
       "       [0.00444156, 0.00397027, 0.00532927],\n",
       "       [0.00588287, 0.00530139, 0.00595906],\n",
       "       [0.00539718, 0.00518115, 0.00605365],\n",
       "       [0.00529195, 0.004936  , 0.00535252],\n",
       "       [0.00455882, 0.00464422, 0.005337  ],\n",
       "       [0.00526366, 0.00511295, 0.00583952],\n",
       "       [0.00269345, 0.00244951, 0.00314537],\n",
       "       [0.00279501, 0.00259441, 0.00333633],\n",
       "       [0.00417321, 0.00388816, 0.00486045],\n",
       "       [0.00262273, 0.0029119 , 0.00287528],\n",
       "       [0.00341066, 0.00310986, 0.00411968],\n",
       "       [0.00881738, 0.00876829, 0.00971992],\n",
       "       [0.00690376, 0.00652364, 0.00723898],\n",
       "       [0.00559128, 0.00565756, 0.0062769 ],\n",
       "       [0.00574137, 0.00604095, 0.00649538],\n",
       "       [0.00681878, 0.00678583, 0.00705083],\n",
       "       [0.00849944, 0.00848692, 0.00926169],\n",
       "       [0.01132243, 0.00893512, 0.01025637],\n",
       "       [0.00591637, 0.00577664, 0.00672626],\n",
       "       [0.00890799, 0.00838015, 0.00918373],\n",
       "       [0.00097849, 0.0013709 , 0.00130826],\n",
       "       [0.01008793, 0.00991889, 0.01049375],\n",
       "       [0.00640723, 0.00526028, 0.00704603],\n",
       "       [0.00410342, 0.00373085, 0.00464673],\n",
       "       [0.00423535, 0.00309299, 0.00486726],\n",
       "       [0.00874699, 0.00724541, 0.00947407],\n",
       "       [0.00908665, 0.00838874, 0.0096695 ],\n",
       "       [0.00462504, 0.00486396, 0.00491644],\n",
       "       [0.00489147, 0.00547311, 0.00552156],\n",
       "       [0.00256887, 0.00280099, 0.00387871],\n",
       "       [0.00641702, 0.00556045, 0.00656304],\n",
       "       [0.00681172, 0.00631756, 0.00716005],\n",
       "       [0.00149766, 0.00141189, 0.00200431],\n",
       "       [0.0025858 , 0.00227181, 0.00324908],\n",
       "       [0.01682652, 0.01524261, 0.0178266 ],\n",
       "       [0.00592161, 0.00523183, 0.00601495],\n",
       "       [0.00958817, 0.0090796 , 0.00989466],\n",
       "       [0.00424663, 0.00424069, 0.00465738],\n",
       "       [0.00207255, 0.00176161, 0.00279499],\n",
       "       [0.00785489, 0.00748363, 0.00809377],\n",
       "       [0.00925519, 0.00889579, 0.0101932 ],\n",
       "       [0.01132504, 0.01113635, 0.01197168],\n",
       "       [0.00509424, 0.00426971, 0.00571169],\n",
       "       [0.00366672, 0.00267683, 0.00378184],\n",
       "       [0.00282827, 0.00351137, 0.00382556],\n",
       "       [0.00501091, 0.00415802, 0.00564297],\n",
       "       [0.00627101, 0.00660901, 0.00718569],\n",
       "       [0.0063758 , 0.0052282 , 0.00594371],\n",
       "       [0.00281812, 0.00206395, 0.00322549],\n",
       "       [0.00671724, 0.00673268, 0.00799858],\n",
       "       [0.00374266, 0.00356568, 0.00409827],\n",
       "       [0.0084406 , 0.00841648, 0.00880869],\n",
       "       [0.00144827, 0.00146876, 0.00185142],\n",
       "       [0.00383673, 0.00282966, 0.0040946 ],\n",
       "       [0.0063613 , 0.00528869, 0.00695197]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "X_test2 = shp_clf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf.fit(X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5666666666666667\n",
      "F1-score [0.48       0.62857143]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.40      0.48        15\n",
      "           1       0.55      0.73      0.63        15\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.57      0.57      0.55        30\n",
      "weighted avg       0.57      0.57      0.55        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5666666666666667\n",
      "F1-score [0.55172414 0.58064516]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.53      0.55        15\n",
      "           1       0.56      0.60      0.58        15\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.57      0.57      0.57        30\n",
      "weighted avg       0.57      0.57      0.57        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train2, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-based Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(values):\n",
    "    features = {\n",
    "        'avg': np.mean(values),\n",
    "        'std': np.std(values),\n",
    "        'var': np.var(values),\n",
    "        'med': np.median(values),\n",
    "        '10p': np.percentile(values, 10),\n",
    "        '25p': np.percentile(values, 25),\n",
    "        '50p': np.percentile(values, 50),\n",
    "        '75p': np.percentile(values, 75),\n",
    "        '90p': np.percentile(values, 90),\n",
    "        'iqr': np.percentile(values, 75) - np.percentile(values, 25),\n",
    "        'cov': 1.0 * np.mean(values) / np.std(values),\n",
    "        'skw': stats.skew(values),\n",
    "        'kur': stats.kurtosis(values)\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = np.array([list(calculate_features(x).values()) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.43927152,  0.27209024,  0.0740331 ,  0.40488432,  0.10025707,\n",
       "         0.19730077,  0.40488432,  0.65070694,  0.81748072,  0.45340617,\n",
       "         1.61443319,  0.22065247, -1.10200899],\n",
       "       [ 0.31167953,  0.22444126,  0.05037388,  0.24444444,  0.10255556,\n",
       "         0.13333333,  0.24444444,  0.44444444,  0.63555556,  0.31111111,\n",
       "         1.38869084,  1.14018974,  0.74900105],\n",
       "       [ 0.29156586,  0.23564912,  0.05553051,  0.20986359,  0.06086044,\n",
       "         0.13641133,  0.20986359,  0.40490556,  0.6239944 ,  0.26849423,\n",
       "         1.23728816,  1.36724864,  1.20918862],\n",
       "       [ 0.36421441,  0.2583849 ,  0.06676276,  0.3225    ,  0.05708333,\n",
       "         0.13125   ,  0.3225    ,  0.56828125,  0.75      ,  0.43703125,\n",
       "         1.40958084,  0.4385618 , -0.94833788],\n",
       "       [ 0.38993345,  0.27252317,  0.07426888,  0.35      ,  0.061875  ,\n",
       "         0.146875  ,  0.35      ,  0.579375  ,  0.7975    ,  0.4325    ,\n",
       "         1.43082679,  0.45976979, -0.8712124 ],\n",
       "       [ 0.37629406,  0.26671497,  0.07113687,  0.32521008,  0.07310924,\n",
       "         0.1512605 ,  0.32521008,  0.55210084,  0.77773109,  0.40084034,\n",
       "         1.41084717,  0.5616004 , -0.73219957],\n",
       "       [ 0.28869474,  0.21303569,  0.04538421,  0.22580645,  0.09677419,\n",
       "         0.12903226,  0.22580645,  0.38252688,  0.5483871 ,  0.25349462,\n",
       "         1.3551473 ,  1.33309795,  1.36714823],\n",
       "       [ 0.27760931,  0.24003327,  0.05761597,  0.2       ,  0.05666667,\n",
       "         0.10833333,  0.2       ,  0.38263889,  0.63044444,  0.27430556,\n",
       "         1.15654515,  1.32473907,  1.15563952],\n",
       "       [ 0.31761803,  0.24655587,  0.0607898 ,  0.2467033 ,  0.08791209,\n",
       "         0.13186813,  0.2467033 ,  0.42857143,  0.715     ,  0.2967033 ,\n",
       "         1.28821929,  1.09366283,  0.32112453],\n",
       "       [ 0.41277343,  0.26157857,  0.06842335,  0.38560411,  0.07455013,\n",
       "         0.17352185,  0.38560411,  0.64267352,  0.79691517,  0.46915167,\n",
       "         1.57800939,  0.26236401, -1.1517362 ],\n",
       "       [ 0.35618673,  0.25415278,  0.06459364,  0.31949153,  0.06525424,\n",
       "         0.13347458,  0.31949153,  0.52971398,  0.7440678 ,  0.39623941,\n",
       "         1.40146699,  0.6291366 , -0.41613793],\n",
       "       [ 0.29467049,  0.22075802,  0.0487341 ,  0.23834968,  0.06096706,\n",
       "         0.14269446,  0.23834968,  0.42571829,  0.54660126,  0.28302383,\n",
       "         1.33481216,  1.15113167,  1.10387329],\n",
       "       [ 0.38436921,  0.25918548,  0.06717711,  0.35      ,  0.0725    ,\n",
       "         0.140625  ,  0.35      ,  0.5759375 ,  0.75      ,  0.4353125 ,\n",
       "         1.482989  ,  0.45019124, -0.77724919],\n",
       "       [ 0.28684439,  0.24016427,  0.05767887,  0.20697168,  0.06318083,\n",
       "         0.11982571,  0.20697168,  0.4248366 ,  0.68409586,  0.30501089,\n",
       "         1.19436747,  1.24622503,  0.80983113],\n",
       "       [ 0.37584877,  0.26003985,  0.06762072,  0.3245283 ,  0.07295597,\n",
       "         0.15801887,  0.3245283 ,  0.5572327 ,  0.77232704,  0.39921384,\n",
       "         1.44535065,  0.59000969, -0.69800275],\n",
       "       [ 0.29562916,  0.23854071,  0.05690167,  0.21348315,  0.07640449,\n",
       "         0.1011236 ,  0.21348315,  0.41966292,  0.63314607,  0.31853933,\n",
       "         1.23932372,  1.21188962,  0.87019131],\n",
       "       [ 0.40772216,  0.29462042,  0.08680119,  0.38129496,  0.0618705 ,\n",
       "         0.14748201,  0.38129496,  0.63992806,  0.84172662,  0.49244604,\n",
       "         1.38388966,  0.37525328, -1.14776264],\n",
       "       [ 0.39002014,  0.25884872,  0.06700266,  0.36797934,  0.07488702,\n",
       "         0.15493867,  0.36797934,  0.5784377 ,  0.77469335,  0.42349903,\n",
       "         1.50674938,  0.35334636, -0.93766268],\n",
       "       [ 0.32829528,  0.27625496,  0.0763168 ,  0.21677328,  0.06183369,\n",
       "         0.10660981,  0.21677328,  0.46335288,  0.85021322,  0.35674307,\n",
       "         1.18837787,  1.03279175, -0.06948806],\n",
       "       [ 0.37396267,  0.27275872,  0.07439732,  0.33625   ,  0.0725    ,\n",
       "         0.1       ,  0.33625   ,  0.563125  ,  0.754375  ,  0.463125  ,\n",
       "         1.37103839,  0.46752531, -0.99308144],\n",
       "       [ 0.37459263,  0.28155597,  0.07927376,  0.32229124,  0.07361399,\n",
       "         0.11042098,  0.32229124,  0.57971014,  0.82539683,  0.46928916,\n",
       "         1.33043753,  0.60216272, -0.84625191],\n",
       "       [ 0.36240419,  0.2589095 ,  0.06703413,  0.33791749,  0.04977079,\n",
       "         0.10478062,  0.33791749,  0.5758568 ,  0.73084479,  0.47107618,\n",
       "         1.39973306,  0.43154981, -0.89751933],\n",
       "       [ 0.30115083,  0.24975038,  0.06237525,  0.22302158,  0.05755396,\n",
       "         0.11510791,  0.22302158,  0.42670863,  0.69451799,  0.31160072,\n",
       "         1.20580727,  1.16030416,  0.4918289 ],\n",
       "       [ 0.39427083,  0.26485783,  0.07014967,  0.36666667,  0.0725    ,\n",
       "         0.15      ,  0.36666667,  0.57479167,  0.77      ,  0.42479167,\n",
       "         1.48861312,  0.34496842, -0.97530808],\n",
       "       [ 0.36340218,  0.27936516,  0.07804489,  0.31923077,  0.03974359,\n",
       "         0.11858974,  0.31923077,  0.58974359,  0.75940171,  0.47115385,\n",
       "         1.30081428,  0.49737587, -0.94034515],\n",
       "       [ 0.33326702,  0.25103651,  0.06301933,  0.30989583,  0.04947917,\n",
       "         0.1171875 ,  0.30989583,  0.49479167,  0.72200521,  0.37760417,\n",
       "         1.32756395,  0.68258661, -0.41395307],\n",
       "       [ 0.3166959 ,  0.24228486,  0.05870195,  0.22964509,  0.08141962,\n",
       "         0.13308977,  0.22964509,  0.43841336,  0.74551148,  0.30532359,\n",
       "         1.30712215,  1.20920085,  0.70129487],\n",
       "       [ 0.38132957,  0.27588021,  0.07610989,  0.35      ,  0.06      ,\n",
       "         0.1       ,  0.35      ,  0.579375  ,  0.775     ,  0.479375  ,\n",
       "         1.38222881,  0.48483246, -0.91329868],\n",
       "       [ 0.27232902,  0.21640993,  0.04683326,  0.20876827,  0.08141962,\n",
       "         0.12265136,  0.20876827,  0.3375087 ,  0.50967293,  0.21485734,\n",
       "         1.25839429,  1.6273862 ,  2.41556113],\n",
       "       [ 0.30823918,  0.25227217,  0.06364125,  0.22884882,  0.07073509,\n",
       "         0.11442441,  0.22884882,  0.44209431,  0.7068828 ,  0.3276699 ,\n",
       "         1.2218517 ,  1.17716999,  0.56018741],\n",
       "       [ 0.32478935,  0.25029028,  0.06264522,  0.22494888,  0.0797546 ,\n",
       "         0.12269939,  0.22494888,  0.44989775,  0.71574642,  0.32719836,\n",
       "         1.29765067,  1.04111914,  0.17543067],\n",
       "       [ 0.31094117,  0.23467284,  0.05507134,  0.22845275,  0.08099688,\n",
       "         0.14719626,  0.22845275,  0.42056075,  0.64382139,  0.27336449,\n",
       "         1.32499852,  1.28406419,  1.11330327],\n",
       "       [ 0.34003245,  0.20951013,  0.04389449,  0.30550622,  0.10625617,\n",
       "         0.18709295,  0.30550622,  0.4928952 ,  0.60864417,  0.30580225,\n",
       "         1.62298816,  0.7793069 ,  0.24932886],\n",
       "       [ 0.30175511,  0.2480712 ,  0.06153932,  0.21321962,  0.06183369,\n",
       "         0.10660981,  0.21321962,  0.40511727,  0.71791045,  0.29850746,\n",
       "         1.21640527,  1.21197841,  0.58869267],\n",
       "       [ 0.32725635,  0.23991378,  0.05755862,  0.26561428,  0.0881606 ,\n",
       "         0.14885038,  0.26561428,  0.4646534 ,  0.68805765,  0.31580302,\n",
       "         1.36405817,  0.99269575,  0.29366892],\n",
       "       [ 0.32544219,  0.25390556,  0.06446803,  0.30512821,  0.02884615,\n",
       "         0.0849359 ,  0.30512821,  0.48717949,  0.75555556,  0.40224359,\n",
       "         1.28174502,  0.70241479, -0.35211673],\n",
       "       [ 0.33499434,  0.25049609,  0.06274829,  0.23404255,  0.08297872,\n",
       "         0.1668883 ,  0.23404255,  0.42553191,  0.76595745,  0.25864362,\n",
       "         1.33732364,  1.1571869 ,  0.42960989],\n",
       "       [ 0.33171173,  0.25440524,  0.06472203,  0.2393617 ,  0.08297872,\n",
       "         0.15611702,  0.2393617 ,  0.45079787,  0.8156383 ,  0.29468085,\n",
       "         1.30387144,  1.16976393,  0.5299118 ],\n",
       "       [ 0.35774482,  0.2516749 ,  0.06334026,  0.31101695,  0.04788136,\n",
       "         0.15847458,  0.31101695,  0.53983051,  0.73813559,  0.38135593,\n",
       "         1.42145609,  0.54271588, -0.55490116],\n",
       "       [ 0.3395218 ,  0.26878032,  0.07224286,  0.28860759,  0.06075949,\n",
       "         0.08860759,  0.28860759,  0.50949367,  0.76012658,  0.42088608,\n",
       "         1.26319444,  0.67108067, -0.68749108],\n",
       "       [ 0.42506493,  0.25600425,  0.06553817,  0.3902439 ,  0.09512195,\n",
       "         0.19268293,  0.3902439 ,  0.62195122,  0.76036585,  0.42926829,\n",
       "         1.66038233,  0.28180219, -0.89148402],\n",
       "       [ 0.32783151,  0.25810117,  0.06661622,  0.22494888,  0.08200409,\n",
       "         0.16155419,  0.22494888,  0.45104806,  0.82515337,  0.28949387,\n",
       "         1.27016666,  1.18298674,  0.39865194],\n",
       "       [ 0.27424473,  0.2228822 ,  0.04967647,  0.20620438,  0.05620438,\n",
       "         0.10629562,  0.20620438,  0.38467153,  0.55036496,  0.27837591,\n",
       "         1.23044699,  1.36311511,  1.40737058],\n",
       "       [ 0.29432376,  0.25105811,  0.06303017,  0.21023125,  0.06096706,\n",
       "         0.10511563,  0.21023125,  0.39943938,  0.68955851,  0.29432376,\n",
       "         1.1723332 ,  1.30998305,  0.90487971],\n",
       "       [ 0.37625723,  0.257655  ,  0.0663861 ,  0.35      ,  0.0725    ,\n",
       "         0.140625  ,  0.35      ,  0.5575    ,  0.75      ,  0.416875  ,\n",
       "         1.4603141 ,  0.40315516, -0.87704381],\n",
       "       [ 0.25860775,  0.1990687 ,  0.03962835,  0.20788913,  0.06183369,\n",
       "         0.12793177,  0.20788913,  0.32515991,  0.48827292,  0.19722814,\n",
       "         1.299088  ,  1.61342949,  2.79182068],\n",
       "       [ 0.38447455,  0.27875253,  0.07770297,  0.36083916,  0.07132867,\n",
       "         0.11188811,  0.36083916,  0.58741259,  0.81608392,  0.47552448,\n",
       "         1.37926838,  0.48121265, -1.00414982],\n",
       "       [ 0.34448599,  0.26499124,  0.07022036,  0.30512821,  0.04871795,\n",
       "         0.07692308,  0.30512821,  0.53589744,  0.74358974,  0.45897436,\n",
       "         1.2999901 ,  0.51380039, -0.83262553],\n",
       "       [ 0.35714017,  0.26928301,  0.07251334,  0.33152665,  0.05149051,\n",
       "         0.10840108,  0.33152665,  0.54652213,  0.8001355 ,  0.43812105,\n",
       "         1.3262633 ,  0.55394712, -0.85533278],\n",
       "       [ 0.42984691,  0.25392094,  0.06447584,  0.39877301,  0.12269939,\n",
       "         0.19386503,  0.39877301,  0.60337423,  0.8007362 ,  0.4095092 ,\n",
       "         1.69283758,  0.3757822 , -0.91513254],\n",
       "       [ 0.33301648,  0.25879839,  0.06697661,  0.22727273,  0.08057851,\n",
       "         0.16322314,  0.22727273,  0.42742769,  0.76291322,  0.26420455,\n",
       "         1.28677956,  1.09101853,  0.16328041],\n",
       "       [ 0.33436156,  0.25576547,  0.06541598,  0.2487906 ,  0.10366275,\n",
       "         0.14979267,  0.2487906 ,  0.47127678,  0.76088459,  0.32148411,\n",
       "         1.30729749,  1.05406875,  0.10797649],\n",
       "       [ 0.30385258,  0.25446154,  0.06475067,  0.2198364 ,  0.0797546 ,\n",
       "         0.12269939,  0.2198364 ,  0.40439673,  0.69406953,  0.28169734,\n",
       "         1.19410023,  1.36659389,  0.97958456],\n",
       "       [ 0.36416726,  0.27006898,  0.07293725,  0.32602151,  0.04903226,\n",
       "         0.11129032,  0.32602151,  0.63870968,  0.7483871 ,  0.52741935,\n",
       "         1.34842317,  0.43780331, -1.06102254],\n",
       "       [ 0.43200893,  0.28305323,  0.08011913,  0.4       ,  0.08285714,\n",
       "         0.18107143,  0.4       ,  0.6275    ,  0.85714286,  0.44642857,\n",
       "         1.52624623,  0.33212029, -1.06715944],\n",
       "       [ 0.30739069,  0.2250106 ,  0.05062977,  0.24444444,  0.08666667,\n",
       "         0.13888889,  0.24444444,  0.40333333,  0.55188889,  0.26444444,\n",
       "         1.36611646,  1.47729786,  1.98932794],\n",
       "       [ 0.31877808,  0.2181653 ,  0.0475961 ,  0.24594746,  0.1117943 ,\n",
       "         0.17160425,  0.24594746,  0.44717719,  0.55651202,  0.27557295,\n",
       "         1.4611768 ,  1.15637943,  1.01206315],\n",
       "       [ 0.37928303,  0.27450172,  0.0753512 ,  0.3299458 ,  0.05149051,\n",
       "         0.13550136,  0.3299458 ,  0.59349593,  0.78590786,  0.45799458,\n",
       "         1.38171459,  0.34140828, -1.07372936],\n",
       "       [ 0.3435515 ,  0.25512952,  0.06509107,  0.33625   ,  0.06      ,\n",
       "         0.1       ,  0.33625   ,  0.5475    ,  0.71883333,  0.4475    ,\n",
       "         1.34657684,  0.48637184, -0.89632965],\n",
       "       [ 0.38965011,  0.28959486,  0.08386518,  0.36111111,  0.04871795,\n",
       "         0.12820513,  0.36111111,  0.65112179,  0.76923077,  0.52291667,\n",
       "         1.34550076,  0.33226023, -1.15708793],\n",
       "       [ 0.32336076,  0.24275412,  0.05892956,  0.23913043,  0.08478261,\n",
       "         0.13043478,  0.23913043,  0.47418478,  0.67391304,  0.34375   ,\n",
       "         1.33205053,  1.10967004,  0.69015014],\n",
       "       [ 0.40090856,  0.2689667 ,  0.07234308,  0.35      ,  0.0725    ,\n",
       "         0.15      ,  0.35      ,  0.60208333,  0.773125  ,  0.45208333,\n",
       "         1.49055094,  0.33804219, -0.96777983],\n",
       "       [ 0.28928807,  0.21001952,  0.0441082 ,  0.24271845,  0.06924615,\n",
       "         0.11422045,  0.24271845,  0.38638635,  0.51759709,  0.27216591,\n",
       "         1.3774342 ,  1.19429698,  1.35758936],\n",
       "       [ 0.29711784,  0.22773835,  0.05186476,  0.22964509,  0.06367432,\n",
       "         0.12526096,  0.22964509,  0.41753653,  0.64091858,  0.29227557,\n",
       "         1.30464559,  1.26326116,  1.19883397],\n",
       "       [ 0.44527124,  0.28104684,  0.07898732,  0.4       ,  0.07247059,\n",
       "         0.18382353,  0.4       ,  0.68235294,  0.83764706,  0.49852941,\n",
       "         1.5843311 ,  0.16126361, -1.1827466 ],\n",
       "       [ 0.28534496,  0.24640585,  0.06071584,  0.2173913 ,  0.06304348,\n",
       "         0.10869565,  0.2173913 ,  0.35027174,  0.69152174,  0.24157609,\n",
       "         1.15802834,  1.40780774,  1.17140177],\n",
       "       [ 0.28801203,  0.23509695,  0.05527058,  0.21195652,  0.08695652,\n",
       "         0.14755435,  0.21195652,  0.34782609,  0.58706522,  0.20027174,\n",
       "         1.22507767,  1.60622847,  2.11677589],\n",
       "       [ 0.33995669,  0.26267126,  0.06899619,  0.30709677,  0.04      ,\n",
       "         0.07741935,  0.30709677,  0.51612903,  0.70744086,  0.43870968,\n",
       "         1.29422873,  0.55538772, -0.66235996],\n",
       "       [ 0.37149508,  0.27087474,  0.07337313,  0.31      ,  0.0725    ,\n",
       "         0.14791667,  0.31      ,  0.5475    ,  0.773125  ,  0.39958333,\n",
       "         1.37146446,  0.56078702, -0.83854464],\n",
       "       [ 0.33177587,  0.24324353,  0.05916741,  0.275     ,  0.06304348,\n",
       "         0.17173913,  0.275     ,  0.44021739,  0.68434783,  0.26847826,\n",
       "         1.36396585,  1.09707429,  0.65602673]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test3 = np.array([list(calculate_features(x).values()) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train3, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test3)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9666666666666667\n",
      "F1-score [0.96551724 0.96774194]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.97        15\n",
      "           1       0.94      1.00      0.97        15\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=8, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.classification import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(metric='dtw_sakoechiba')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, Activation, Conv1D, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_cnn(n_timesteps, n_outputs):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, 1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  144\n",
      "N. LABELS:  2\n"
     ]
    }
   ],
   "source": [
    "X_train_cnn = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train_cnn, y_train, test_size=0.2, stratify=y_train)\n",
    "\n",
    "n_timesteps, n_outputs, n_features = X_train_cnn.shape[1], len(np.unique(y_train_cnn)), 1 \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS: \", n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = build_simple_cnn(n_timesteps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 137, 16)           144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 137, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 137, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 137, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 133, 32)           2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 133, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 133, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 133, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 131, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 131, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 131, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 131, 64)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 9,522\n",
      "Trainable params: 9,298\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train_cnn.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/5\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.6980 - accuracy: 0.5000 - val_loss: 0.6904 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.6560 - accuracy: 0.7143 - val_loss: 0.6885 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.6348 - accuracy: 0.6786 - val_loss: 0.6871 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.6110 - accuracy: 0.65 - 0s 3ms/step - loss: 0.6156 - accuracy: 0.6429 - val_loss: 0.6863 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5828 - accuracy: 0.7143 - val_loss: 0.6867 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history_cnn = cnn.fit(X_train_cnn, y_train_cnn, epochs=5, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5\n",
      "F1-score [0.         0.66666667]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn.predict(X_test_cnn), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6870226263999939, 0.5]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(n_timesteps, n_outputs):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(n_timesteps, 1)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = build_lstm(n_timesteps, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256)               264192    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 280,770\n",
      "Trainable params: 280,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5908 - accuracy: 0.7143 - val_loss: 0.6860 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5274 - accuracy: 0.8214 - val_loss: 0.6863 - val_accuracy: 0.5714\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5356 - accuracy: 0.8036 - val_loss: 0.6880 - val_accuracy: 0.6429\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 0.7679 - val_loss: 0.6885 - val_accuracy: 0.7143\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.5028 - accuracy: 0.7500 - val_loss: 0.6863 - val_accuracy: 0.6429\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4911 - accuracy: 0.8036 - val_loss: 0.6880 - val_accuracy: 0.5714\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.9643 - val_loss: 0.6882 - val_accuracy: 0.5714\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.7857 - val_loss: 0.6872 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.9107 - val_loss: 0.6851 - val_accuracy: 0.5714\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 0s 3ms/step - loss: 0.2934 - accuracy: 0.9821 - val_loss: 0.6821 - val_accuracy: 0.5714\n"
     ]
    }
   ],
   "source": [
    "history_lstm = cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                       validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5\n",
      "F1-score [0.66666667 0.        ]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67        15\n",
      "           1       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm.predict(X_test_cnn), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "30/30 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6950158476829529, 0.5]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.evaluate(X_test_cnn, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyts.datasets import load_basic_motions\n",
    "\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 144, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "ts_list = []\n",
    "for temp in week_list:\n",
    "    ts = temp[['Temperature','Humidity','CO2','Light','HumidityRatio']].copy()\n",
    "    ts_list.append(ts)\n",
    "\n",
    "ts_array = []\n",
    "ts_size = len(week_list[1])\n",
    "for ts in ts_list:\n",
    "    ts_array.append(np.array(ts))\n",
    "    \n",
    "X = np.array(ts_array)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = TimeSeriesScalerMinMax()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 144, 5) (30, 144, 5)\n",
      "[[ 0  1]\n",
      " [35 35]]\n",
      "[[ 0  1]\n",
      " [15 15]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))\n",
    "\n",
    "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTEPS:  144\n",
      "N. LABELS:  2\n",
      "N. FEATURES:  5\n"
     ]
    }
   ],
   "source": [
    "n_timesteps, n_outputs, n_features = X_train.shape[1], len(np.unique(y_train)), X_train.shape[2] \n",
    "print(\"TIMESTEPS: \", n_timesteps)\n",
    "print(\"N. LABELS: \", n_outputs)\n",
    "print(\"N. FEATURES: \", n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn, X_val_cnn, y_train_cnn, y_val_cnn = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, Conv2D, MaxPool2D, Flatten, Dropout, LeakyReLU, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm2(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4, input_shape=(n_timesteps, n_features), return_sequences=True, \n",
    "                        kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #1\n",
    "    for _ in range(2):\n",
    "        model.add(LSTM(4, kernel_initializer='TruncatedNormal', return_sequences=True))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.04))   \n",
    "\n",
    "    #2\n",
    "    model.add(LSTM(32, kernel_initializer='TruncatedNormal', return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "    \n",
    "    #3\n",
    "    for _ in range(2):\n",
    "        model.add(Dense(256, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))\n",
    "    #4\n",
    "    for _ in range(1):\n",
    "        model.add(Dense(64, kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.7))\n",
    "\n",
    "    #5\n",
    "    model.add(Dense(32, kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm2 = build_lstm2(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 144, 4)            160       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 144, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 144, 4)            144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 144, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 144, 4)            144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 144, 4)            16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 144, 4)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 32)                4736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 100,626\n",
      "Trainable params: 99,322\n",
      "Non-trainable params: 1,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_lstm2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/50\n",
      "21/56 [==========>...................] - ETA: 0s - loss: 0.6935 - accuracy: 0.5714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 7ms/step - loss: 0.7151 - accuracy: 0.5536 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.5400 - accuracy: 0.7679 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 1s 9ms/step - loss: 0.4526 - accuracy: 0.7857 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.4107 - accuracy: 0.8929 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.3206 - accuracy: 0.9643 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 1s 9ms/step - loss: 0.2992 - accuracy: 0.9107 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.4015 - accuracy: 0.8929 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.2962 - accuracy: 0.8929 - val_loss: 0.6933 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.2200 - accuracy: 0.9821 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.2034 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.2530 - accuracy: 0.9286 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.1344 - accuracy: 1.0000 - val_loss: 0.6942 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.2331 - accuracy: 0.9107 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.1585 - accuracy: 0.9643 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.1637 - accuracy: 0.9821 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1178 - accuracy: 0.9643 - val_loss: 0.6961 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1485 - accuracy: 0.9821 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.2117 - accuracy: 0.9286 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1689 - accuracy: 0.9464 - val_loss: 0.6956 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.2584 - accuracy: 0.8750 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0800 - accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 1s 11ms/step - loss: 0.1650 - accuracy: 0.9464 - val_loss: 0.6924 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1477 - accuracy: 1.0000 - val_loss: 0.6911 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1295 - accuracy: 0.9821 - val_loss: 0.6907 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 0.6918 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1058 - accuracy: 1.0000 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0741 - accuracy: 1.0000 - val_loss: 0.6997 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0630 - accuracy: 1.0000 - val_loss: 0.7029 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.1773 - accuracy: 0.9821 - val_loss: 0.7005 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0752 - accuracy: 1.0000 - val_loss: 0.6991 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0897 - accuracy: 0.9821 - val_loss: 0.7030 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0972 - accuracy: 1.0000 - val_loss: 0.7092 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0876 - accuracy: 0.9643 - val_loss: 0.7103 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.1791 - accuracy: 0.9464 - val_loss: 0.7005 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 1.0000 - val_loss: 0.6994 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.2137 - accuracy: 0.9107 - val_loss: 0.6823 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9643 - val_loss: 0.7092 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0947 - accuracy: 0.9643 - val_loss: 0.7287 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.1929 - accuracy: 0.9464 - val_loss: 0.7815 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0631 - accuracy: 1.0000 - val_loss: 0.8212 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0571 - accuracy: 1.0000 - val_loss: 0.8461 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.8719 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 0.8880 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 0s 7ms/step - loss: 0.0612 - accuracy: 0.9821 - val_loss: 0.8932 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 0s 8ms/step - loss: 0.0566 - accuracy: 1.0000 - val_loss: 0.9000 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.2868 - accuracy: 0.9464 - val_loss: 0.9128 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0708 - accuracy: 0.9821 - val_loss: 0.9006 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.4080 - accuracy: 0.9107 - val_loss: 0.8485 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.2233 - accuracy: 0.9107 - val_loss: 0.8554 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 1s 10ms/step - loss: 0.0965 - accuracy: 0.9821 - val_loss: 0.9298 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history_lstm2 = lstm2.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                          validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5\n",
      "F1-score [0.         0.66666667]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(lstm2.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 144, 5)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 144, 5, 1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cnn2 = X_train_cnn.reshape(X_train_cnn.shape[0], X_train_cnn.shape[1], X_train_cnn.shape[2], 1)\n",
    "X_val_cnn2 = X_val_cnn.reshape(X_val_cnn.shape[0], X_val_cnn.shape[1], X_val_cnn.shape[2], 1)\n",
    "X_test_cnn2 = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "X_train_cnn2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn2(n_timesteps, n_features, n_outputs):\n",
    "    input_shape = (n_timesteps, n_features, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ks1_first = 3\n",
    "    ks1_second = 3\n",
    "    \n",
    "    ks2_first = 4\n",
    "    ks2_second = 4\n",
    "    \n",
    "    model.add(Conv2D(filters=(3), \n",
    "                     kernel_size=(ks1_first, ks1_second),\n",
    "                     input_shape=input_shape, \n",
    "                     padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.02))\n",
    "    \n",
    "    for _ in range(2):\n",
    "        model.add(Conv2D(filters=(4), \n",
    "                     kernel_size= (ks2_first, ks2_second), \n",
    "                         padding='same',\n",
    "                     kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.2))  \n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    for _ in range(4):\n",
    "        model.add(Dense(64 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.4))\n",
    "    \n",
    "    for _ in range(3):\n",
    "        model.add(Dense(128 , kernel_initializer='TruncatedNormal'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU())\n",
    "        model.add(Dropout(0.3))\n",
    "  \n",
    "    model.add(Dense(1024 , kernel_initializer='TruncatedNormal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.7))\n",
    "        \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = build_cnn2(n_timesteps, n_features, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 144, 5, 3)         30        \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 144, 5, 3)         12        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 144, 5, 3)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 144, 5, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 144, 5, 4)         196       \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 144, 5, 4)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 144, 5, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 144, 5, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 144, 5, 4)         260       \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 144, 5, 4)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 144, 5, 4)         0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 144, 5, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2880)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                184384    \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 379,540\n",
      "Trainable params: 376,190\n",
      "Non-trainable params: 3,350\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.8828 - accuracy: 0.4821 - val_loss: 0.6932 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.8190 - accuracy: 0.5357 - val_loss: 0.6935 - val_accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.7175 - accuracy: 0.5000 - val_loss: 0.6944 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.5365 - accuracy: 0.7857 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8036 - val_loss: 0.6971 - val_accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.7857 - val_loss: 0.6999 - val_accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3667 - accuracy: 0.8571 - val_loss: 0.7033 - val_accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.7679 - val_loss: 0.7064 - val_accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3788 - accuracy: 0.8571 - val_loss: 0.7058 - val_accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.9107 - val_loss: 0.7040 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1297 - accuracy: 0.9821 - val_loss: 0.7044 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.8571 - val_loss: 0.7052 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9464 - val_loss: 0.7063 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1353 - accuracy: 0.9464 - val_loss: 0.7059 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9464 - val_loss: 0.7052 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.9286 - val_loss: 0.7040 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1954 - accuracy: 0.8929 - val_loss: 0.7034 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1473 - accuracy: 0.9464 - val_loss: 0.7023 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9821 - val_loss: 0.7009 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1395 - accuracy: 0.9286 - val_loss: 0.6959 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4246 - accuracy: 0.9286 - val_loss: 0.6919 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0878 - accuracy: 0.9821 - val_loss: 0.6884 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.9107 - val_loss: 0.6878 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2051 - accuracy: 0.9286 - val_loss: 0.6846 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.9464 - val_loss: 0.6790 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0730 - accuracy: 0.9821 - val_loss: 0.6754 - val_accuracy: 0.5714\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9464 - val_loss: 0.6664 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1381 - accuracy: 0.9286 - val_loss: 0.6546 - val_accuracy: 0.9286\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0818 - accuracy: 0.9643 - val_loss: 0.6379 - val_accuracy: 0.5714\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9107 - val_loss: 0.6241 - val_accuracy: 0.6429\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9286 - val_loss: 0.6146 - val_accuracy: 0.9286\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1365 - accuracy: 0.9464 - val_loss: 0.6115 - val_accuracy: 0.7857\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1211 - accuracy: 0.9286 - val_loss: 0.6147 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9821 - val_loss: 0.6132 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0753 - accuracy: 0.9643 - val_loss: 0.6265 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9821 - val_loss: 0.6514 - val_accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1683 - accuracy: 0.9286 - val_loss: 0.6662 - val_accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.9107 - val_loss: 0.6336 - val_accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9821 - val_loss: 0.6093 - val_accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9643 - val_loss: 0.6183 - val_accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9464 - val_loss: 0.6381 - val_accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.5336 - accuracy: 0.8929 - val_loss: 0.6201 - val_accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0817 - accuracy: 0.9643 - val_loss: 0.6116 - val_accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2110 - accuracy: 0.9286 - val_loss: 0.6175 - val_accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 1.0000 - val_loss: 0.5913 - val_accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.5778 - val_accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9821 - val_loss: 0.5699 - val_accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1874 - accuracy: 0.9286 - val_loss: 0.5760 - val_accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1382 - accuracy: 0.9821 - val_loss: 0.5513 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history_cnn2 = cnn2.fit(X_train_cnn2, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn2, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.5\n",
      "F1-score [0.         0.66666667]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.50        30\n",
      "   macro avg       0.25      0.50      0.33        30\n",
      "weighted avg       0.25      0.50      0.33        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn2.predict(X_test_cnn2), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn3(n_timesteps, n_outputs, n_features):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(filters=16, kernel_size=8, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=32, kernel_size=5, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    \n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = build_cnn3(n_timesteps, n_outputs, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 137, 16)           656       \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 137, 16)           64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 137, 16)           0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 137, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 133, 32)           2592      \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 133, 32)           128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 133, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 133, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 131, 64)           6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 131, 64)           256       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 131, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 131, 64)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 10,034\n",
      "Trainable params: 9,810\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
    "mc = ModelCheckpoint('best_model_cnn2.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks = [rlr, mc]\n",
    "\n",
    "batch_size = 16\n",
    "mini_batch_size = int(min(X_train.shape[0]/10, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56 samples, validate on 14 samples\n",
      "Epoch 1/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.6809 - accuracy: 0.5536 - val_loss: 0.6821 - val_accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "42/56 [=====================>........] - ETA: 0s - loss: 0.5483 - accuracy: 0.8095"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/theano/tensor/subtensor.py:2339: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  out[0][inputs[2:]] = inputs[1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.8571 - val_loss: 0.6745 - val_accuracy: 0.5714\n",
      "Epoch 3/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 1.0000 - val_loss: 0.6649 - val_accuracy: 0.9286\n",
      "Epoch 4/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2047 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 1.0000\n",
      "Epoch 5/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.2705 - accuracy: 0.9286 - val_loss: 0.6413 - val_accuracy: 1.0000\n",
      "Epoch 6/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 1.0000 - val_loss: 0.6375 - val_accuracy: 1.0000\n",
      "Epoch 7/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0880 - accuracy: 1.0000 - val_loss: 0.6364 - val_accuracy: 0.7857\n",
      "Epoch 8/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.6429 - val_accuracy: 0.7857\n",
      "Epoch 9/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0758 - accuracy: 1.0000 - val_loss: 0.6467 - val_accuracy: 0.6429\n",
      "Epoch 10/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 1.0000 - val_loss: 0.6497 - val_accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9821 - val_loss: 0.6651 - val_accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.6703 - val_accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 0.6640 - val_accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.6511 - val_accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.6538 - val_accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 0.6588 - val_accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 0.6615 - val_accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.6648 - val_accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6684 - val_accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0665 - accuracy: 0.9821 - val_loss: 0.6485 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.6380 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.6340 - val_accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.6310 - val_accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.6275 - val_accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.6110 - val_accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0494 - accuracy: 0.9821 - val_loss: 0.6072 - val_accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.5843 - val_accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5683 - val_accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.5530 - val_accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.5401 - val_accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.5329 - val_accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.5259 - val_accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.5193 - val_accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.5182 - val_accuracy: 0.5714\n",
      "Epoch 37/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.5046 - val_accuracy: 0.5714\n",
      "Epoch 38/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4921 - val_accuracy: 0.5714\n",
      "Epoch 39/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.4837 - val_accuracy: 0.5714\n",
      "Epoch 40/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.4711 - val_accuracy: 0.7143\n",
      "Epoch 41/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.4553 - val_accuracy: 0.9286\n",
      "Epoch 42/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.4419 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9821 - val_loss: 0.3884 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2976 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.2586 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.2477 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.2497 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.2523 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "56/56 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.2447 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history_cnn3 = cnn3.fit(X_train_cnn, y_train_cnn, epochs=50, batch_size=mini_batch_size, callbacks=callbacks,\n",
    "                      validation_data=(X_val_cnn, y_val_cnn)).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 1.0\n",
      "F1-score [1. 1.]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00        15\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(cnn3.predict(X_test), axis=1)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pyts.readthedocs.io/en/stable/generated/pyts.multivariate.classification.MultivariateClassifier.html#pyts.multivariate.classification.MultivariateClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
